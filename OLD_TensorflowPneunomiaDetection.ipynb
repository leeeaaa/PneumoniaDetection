{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import pandas as pd       \n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt    \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "# this may not work with old tensorflow versions\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"scaled_chest_xray\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "IMAGE_SIZE = [180, 180]\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "IMAGE_CROP = 1\n",
    "RANDOM_STATE = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_CNN = tf.io.gfile.glob(str(train_dir + '/*/*'))\n",
    "filenames_CNN.extend(tf.io.gfile.glob(str(test_dir + '/*/*')))\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets.\n",
    "t_filenames_CNN, test_filenames_CNN = train_test_split(filenames_CNN, test_size=0.2, random_state = RANDOM_STATE)\n",
    "train_filenames_CNN, val_filenames_CNN = train_test_split(t_filenames_CNN, test_size=0.2, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_NORMAL_train_CNN = len([filename for filename in train_filenames_CNN if \"NORMAL\" in filename])\n",
    "print(\"Normal images count in training set: \" + str(COUNT_NORMAL_train_CNN))\n",
    "\n",
    "COUNT_PNEUMONIA_train_CNN = len([filename for filename in train_filenames_CNN if \"PNEUMONIA\" in filename])\n",
    "print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA_train_CNN))\n",
    "print(\"Sum: \" + str(len(train_filenames_CNN)))\n",
    "print('---------------------------')\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "COUNT_NORMAL_val_CNN = len([filename for filename in val_filenames_CNN if \"NORMAL\" in filename])\n",
    "print(\"Normal images count in validation set: \" + str(COUNT_NORMAL_val_CNN))\n",
    "\n",
    "COUNT_PNEUMONIA_val_CNN = len([filename for filename in val_filenames_CNN if \"PNEUMONIA\" in filename])\n",
    "print(\"Pneumonia images count in validation set: \" + str(COUNT_PNEUMONIA_val_CNN))\n",
    "print(\"Sum: \" + str(len(val_filenames_CNN)))\n",
    "print('---------------------------')\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "COUNT_NORMAL_test_CNN = len([filename for filename in test_filenames_CNN if \"NORMAL\" in filename])\n",
    "print(\"Normal images count in test set: \" + str(COUNT_NORMAL_test_CNN))\n",
    "\n",
    "COUNT_PNEUMONIA_test_CNN = len([filename for filename in test_filenames_CNN if \"PNEUMONIA\" in filename])\n",
    "print(\"Pneumonia images count in test set: \" + str(COUNT_PNEUMONIA_test_CNN))\n",
    "print(\"Sum: \" + str(len(test_filenames_CNN)) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(n,p,name):\n",
    "    X_axis = np.arange(len(name))\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    \n",
    "    plt.bar(X_axis - 0.2, n, 0.4, label = 'Normal(0)')\n",
    "    plt.bar(X_axis + 0.2, p, 0.4, label = 'Pneumonia(1)')\n",
    "\n",
    "    plt.xticks(X_axis, name)\n",
    "    plt.xlabel('Sets', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.title('Number of cases in sets', fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results \n",
    "plot_dataset([COUNT_NORMAL_train_CNN, COUNT_NORMAL_val_CNN, COUNT_NORMAL_test_CNN], [COUNT_PNEUMONIA_train_CNN, COUNT_PNEUMONIA_val_CNN, COUNT_PNEUMONIA_test_CNN], ['train', 'validation', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_ds_CNN = tf.data.Dataset.from_tensor_slices(train_filenames_CNN)\n",
    "val_list_ds_CNN = tf.data.Dataset.from_tensor_slices(val_filenames_CNN)\n",
    "test_list_ds_CNN = tf.data.Dataset.from_tensor_slices(test_filenames_CNN)\n",
    "\n",
    "print('Some example filenames: \\n')\n",
    "for f in train_list_ds_CNN.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_COUNT_CNN = tf.data.experimental.cardinality(train_list_ds_CNN).numpy()\n",
    "print(\"Training images count: \" + str(TRAIN_IMG_COUNT_CNN))\n",
    "\n",
    "VAL_IMG_COUNT_CNN = tf.data.experimental.cardinality(val_list_ds_CNN).numpy()\n",
    "print(\"Validating images count: \" + str(VAL_IMG_COUNT_CNN))\n",
    "\n",
    "TEST_IMG_COUNT_CNN = tf.data.experimental.cardinality(test_list_ds_CNN).numpy()\n",
    "print(\"Testing images count: \" + str(TEST_IMG_COUNT_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == \"PNEUMONIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that applies Gaussian Noise to the images.\n",
    "'''\n",
    "def add_gaussian_noise(img):\n",
    "    # image must be scaled in [0, 1]\n",
    "    with tf.name_scope('Add_gaussian_noise'):\n",
    "        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=(200)/(255), dtype=tf.float32)\n",
    "        noise_img = img + noise\n",
    "        noise_img = tf.clip_by_value(noise_img, 0.0, 1.0)\n",
    "    return noise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # For keeping only a portion of the image\n",
    "    img = tf.image.central_crop(img, IMAGE_CROP)\n",
    "    # For adding some noise\n",
    "    #img = add_gaussian_noise(img)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_CNN = train_list_ds_CNN.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds_CNN = val_list_ds_CNN.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds_CNN = test_list_ds_CNN.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_ds_CNN.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  print(\"Label: \", label.numpy())\n",
    "  plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "      if isinstance(cache, str):\n",
    "          ds = ds.cache(cache)\n",
    "      else:\n",
    "          ds = ds.cache()\n",
    "\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_CNN = prepare_for_training(train_ds_CNN)\n",
    "val_ds_CNN = prepare_for_training(val_ds_CNN)\n",
    "test_ds_CNN = test_ds_CNN.batch(BATCH_SIZE)\n",
    "\n",
    "image_batch_CNN, label_batch_CNN = next(iter(train_ds_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.7),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / COUNT_NORMAL_train_CNN) * (TRAIN_IMG_COUNT_CNN / 2.0)\n",
    "weight_for_1 = (1 / COUNT_PNEUMONIA_train_CNN) * (TRAIN_IMG_COUNT_CNN / 2.0)\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=METRICS\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds_CNN,\n",
    "    steps_per_epoch=TRAIN_IMG_COUNT_CNN // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds_CNN,\n",
    "    validation_steps=VAL_IMG_COUNT_CNN // BATCH_SIZE,\n",
    "    class_weight=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xRayNetNN.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "  patience=10,\n",
    "  restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "  k = 0.1\n",
    "  lrate = initial_learning_rate * np.exp(-k*epoch)\n",
    "  return lrate\n",
    "\n",
    "lr_schedule_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds_CNN,\n",
    "    steps_per_epoch=TRAIN_IMG_COUNT_CNN // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds_CNN,\n",
    "    validation_steps=VAL_IMG_COUNT_CNN // BATCH_SIZE,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[checkpoint_cb, earlyStopping_cb, lr_schedule_cb],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['precision', 'recall', 'accuracy', 'loss']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_CNN = (model.predict(test_ds_CNN, batch_size=16) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the original labels of each image\n",
    "orig_test_labels = []\n",
    "for image, label in test_ds_CNN.as_numpy_iterator():\n",
    "    for x in label:\n",
    "        orig_test_labels.append(x)\n",
    "print(np.array(orig_test_labels).shape)\n",
    "print(np.array(preds_CNN).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(orig_test_labels))\n",
    "print(np.array(preds_CNN).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix\n",
    "cm_CNN  = confusion_matrix(orig_test_labels, preds_CNN)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_CNN,figsize=(10,6), hide_ticks=True,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_CNN, acc_CNN, prec_CNN, rec_CNN = model.evaluate(test_ds_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluate function calculating...')\n",
    "print(\"Recall of the model is {:.3f}\".format(rec_CNN))\n",
    "print(\"Precision of the model is {:.3f}\".format(prec_CNN))\n",
    "\n",
    "# Checking if the results are correct by manually calculating Precision and Recall with confusion matrix results\n",
    "print('\\nManually calculating...')\n",
    "tn, fp, fn, tp = cm_CNN.ravel()\n",
    "\n",
    "precision_CNN = tp/(tp+fp)\n",
    "recall_CNN = tp/(tp+fn)\n",
    "\n",
    "print(\"Recall of the model is {:.3f}\".format(recall_CNN))\n",
    "print(\"Precision of the model is {:.3f}\".format(precision_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c15c83bb13a6661006366ccb85eabb9cb3c78a2a67451438f95b14adcf94997"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
