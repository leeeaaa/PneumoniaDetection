{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "NORMAL_OUTPUT_VALUE = 0\n",
    "PNEUMONIA_OUTPUT_VALUE = 1\n",
    "SEED = 2\n",
    "WIDTH=56\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x - A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s - sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x - A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s - relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Collect all the paths of the scaled images and load them into the program\n",
    "    as numpy arrays.\n",
    "\n",
    "    Hint: The original images from https://www.kaggle.com/datasets/tolgadincer/labeled-chest-xray-images\n",
    "    have been downscaled with the sperate python script 'PythonImageScaler.py'\n",
    "\n",
    "    Returns:\n",
    "    train_X - All training images\n",
    "    train_Y - All true outputs of all training images\n",
    "    test_X - All test images\n",
    "    test_Y - All true outputs of all test images\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the root directories of the train and test sets.\n",
    "    scaled_train_dir = \"scaled2_chest_xray/train/\"\n",
    "    scaled_test_dir = \"scaled2_chest_xray/test/\"\n",
    "\n",
    "    # Define the root directories of the normal and pneumonia \n",
    "    # test and trainsets.\n",
    "    scaled_train_pneu = os.listdir(os.path.join(scaled_train_dir, 'PNEUMONIA'))\n",
    "    scaled_train_normal = os.listdir(os.path.join(scaled_train_dir, 'NORMAL'))\n",
    "    scaled_test_pneu = os.listdir(os.path.join(scaled_test_dir, 'PNEUMONIA'))\n",
    "    scaled_test_normal = os.listdir(os.path.join(scaled_test_dir, 'NORMAL'))\n",
    "\n",
    "    # Collect all paths from the training images and test images\n",
    "    # and combine these with the true output value (either \n",
    "    # NORMAL_OUTPUT_VALUE or PNEUMONIA_OUTPUT_VALUE)\n",
    "    train_set_image_paths = [('PNEUMONIA/' + name, PNEUMONIA_OUTPUT_VALUE) for name in scaled_train_pneu] + [('NORMAL/' + name, NORMAL_OUTPUT_VALUE) for name in scaled_train_normal]\n",
    "    test_set_image_paths = [('PNEUMONIA/' + name, PNEUMONIA_OUTPUT_VALUE) for name in scaled_test_pneu] + [('NORMAL/' + name, NORMAL_OUTPUT_VALUE) for name in scaled_test_normal]\n",
    "\n",
    "    # Shuffle the lists\n",
    "    random.shuffle(train_set_image_paths) \n",
    "    random.shuffle(test_set_image_paths) \n",
    "\n",
    "    # Open all image paths and read all the grayscale data from every\n",
    "    # grayscale image. \n",
    "    # \".convert('L')\" makes sure that the image is\n",
    "    # definitely a grayscale, since some images have a few rgb\n",
    "    # values within the image, which caused problems.\n",
    "    test_X_list = [list(Image.open(scaled_test_dir + image_path).getdata().convert('L')) for image_path, i in test_set_image_paths]\n",
    "    train_X_list = [list(Image.open(scaled_train_dir + image_path).getdata().convert('L')) for image_path, i in train_set_image_paths]\n",
    "\n",
    "    # Downscale the grayscale value for a pixel from [0;255] to [0;1]\n",
    "    test_X_list = [[float(value)/255 for value in image_data] for image_data in test_X_list]\n",
    "    train_X_list = [[float(value)/255 for value in image_data] for image_data in train_X_list]\n",
    "\n",
    "    # Convert both  the test and train lists to actual numpy arrays\n",
    "    train_X = np.array(train_X_list, dtype=float).T\n",
    "    test_X = np.array(test_X_list, dtype=float).T\n",
    "    train_Y = np.array([[float(i) for image_path, i in train_set_image_paths]], dtype=float)\n",
    "    test_Y = np.array([[float(i) for image_path, i in test_set_image_paths]], dtype=float)\n",
    "\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12156863 0.47843137 0.         ... 0.01960784 0.04705882 0.45098039]\n",
      " [0.13333333 0.37647059 0.03137255 ... 0.61176471 0.         0.61176471]\n",
      " [0.13333333 0.3254902  0.         ... 0.58039216 0.43137255 0.58431373]\n",
      " ...\n",
      " [0.15686275 0.07058824 0.         ... 0.1254902  0.         0.22352941]\n",
      " [0.15294118 0.09019608 0.01960784 ... 0.14509804 0.         0.        ]\n",
      " [0.14901961 0.10196078 0.         ... 0.11764706 0.01960784 0.02745098]]\n",
      "[[0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      "  0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      "  0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      "  1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      "  0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      "  0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      "  1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      "  0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      "  0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      "  0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      "  0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      "  0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      "  1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      "  1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.]]\n",
      "(3136, 5232)\n",
      "(1, 5232)\n",
      "(3136, 624)\n",
      "(1, 624)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2196f4f59c0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "#print(train_X[0:3])\n",
    "#print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "\n",
    "index = 10\n",
    "plt.imshow(test_X.T[index].reshape(WIDTH, WIDTH), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize all parameters of the neural network based on each layer dimension given\n",
    "    in layer_dims. \n",
    "\n",
    "    Arguments:\n",
    "    layer_dims - python array (list) containing the dimensions of each layer in the network\n",
    "    \n",
    "    Returns:\n",
    "    parameters - python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "        W1 - weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        b1 - bias vector of shape (layer_dims[l], 1)\n",
    "        Wl - weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "        bl - bias vector of shape (1, layer_dims[l])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    parameters = {}\n",
    "    # Number of layers in the network\n",
    "    number_of_layers = len(layer_dims)\n",
    "    \n",
    "    # Generate the all weight and biases matrices for the\n",
    "    # Neural Network based of the values and number of values\n",
    "    # in layer_dims\n",
    "    print(\"Generating a network with the following shapes of the weights and biases:\")\n",
    "    for layer in range(1, number_of_layers):\n",
    "        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1])*0.01\n",
    "        parameters['b'  + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "        \n",
    "        # Print all shapes from all parameters\n",
    "        print(\"{}:{}\".format('W' + str(layer), parameters['W' + str(layer)].shape))\n",
    "        print(\"{}:{}\".format('b' + str(layer), parameters['b' + str(layer)].shape))\n",
    "\n",
    "        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))\n",
    "        assert(parameters['b'  + str(layer)].shape == (layer_dims[layer], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Propagate input features through the neural network.\n",
    "    \n",
    "    Argument:\n",
    "    X - Input features\n",
    "    parameters - Python dictionary containing all parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 - The sigmoid output of the last activation layer\n",
    "    cache - a dictionary containing all values of \"A1\", \"Z1, ... \"AL\", \"ZL\"\n",
    "    \"\"\"\n",
    "    \n",
    "    cache = {}\n",
    "    # Number of layers in the network\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    last_A = X\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        # Calculate Z and A of current layer\n",
    "        Z = np.matmul(parameters['W'+ str(layer)], last_A) + parameters['b' + str(layer)]\n",
    "        A = relu(Z)\n",
    "        # Update cache with Z and A from current layer\n",
    "        cache.update({'Z' + str(layer): Z})\n",
    "        cache.update({'A' + str(layer): A})\n",
    "        # Temporaily save A to use in the next iteration\n",
    "        last_A = A \n",
    "\n",
    "    Z = np.matmul(parameters['W' + str(number_of_layers+1)], last_A) + parameters['b' + str(number_of_layers+1)]\n",
    "    A = sigmoid(Z)\n",
    "    cache.update({'Z' + str(number_of_layers+1): Z})\n",
    "    cache.update({'A' + str(number_of_layers+1): A})\n",
    "    \n",
    "    assert(A.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_probability = 0.5):\n",
    "    \"\"\"\n",
    "    Propagate input features through the neural network.\n",
    "    \n",
    "    Argument:\n",
    "    X - Input features\n",
    "    parameters - Python dictionary containing all parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 - The sigmoid output of the last activation layer\n",
    "    cache - a dictionary containing all values of \"A1\", \"Z1, ... \"AL\", \"ZL\"\n",
    "    \"\"\"\n",
    "    \n",
    "    cache = {}\n",
    "    # Number of layers in the network\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    last_A = X\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        # Calculate Z and A of current layer\n",
    "        Z = np.matmul(parameters['W'+ str(layer)], last_A) + parameters['b' + str(layer)]\n",
    "        A = relu(Z)\n",
    "        # Droput\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_probability).astype(int)\n",
    "        A = A * D\n",
    "        A = A / keep_probability\n",
    "        # Update cache with D, Z and A from current layer\n",
    "        cache.update({'D' + str(layer): D})\n",
    "        cache.update({'Z' + str(layer): Z})\n",
    "        cache.update({'A' + str(layer): A})\n",
    "        # Temporaily save A to use in the next iteration\n",
    "        last_A = A \n",
    "\n",
    "    Z = np.matmul(parameters['W' + str(number_of_layers+1)], last_A) + parameters['b' + str(number_of_layers+1)]\n",
    "    A = sigmoid(Z)\n",
    "    cache.update({'Z' + str(number_of_layers+1): Z})\n",
    "    cache.update({'A' + str(number_of_layers+1): A})\n",
    "    \n",
    "    assert(A.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    epsilon = 1e-7\n",
    "    cost = -(1/m)* float(np.dot(Y, np.log(A.T + epsilon)) + np.dot(1-Y,np.log(1-A.T + epsilon)))\n",
    "    \n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # This gives you the cross-entropy part of the cost\n",
    "    cross_entropy_cost = compute_cost(A, Y)\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 line)\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    sum_of_all_weights = 0.0\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        sum_of_all_weights = sum_of_all_weights + np.sum(np.square(parameters['W' + str(layer)]))\n",
    "\n",
    "    L2_regularization_cost = lambd/(2*m) * sum_of_all_weights\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    ### END CODER HERE ###\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*np.matmul(dZ,cache['A' + str(number_of_layers)].T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*np.matmul(dZ,cache['A' + str(layer-1)].T)\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(parameters, cache, X, Y, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*(np.matmul(dZ,cache['A' + str(number_of_layers)].T)+lambd*parameters['W' + str(number_of_layers+1)])\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*(np.matmul(dZ,cache['A' + str(layer-1)].T)+lambd*parameters['W' + str(layer)])\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)    #(1,1)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*(np.matmul(dZ1,X.T)+lambd*parameters['W1'])\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(parameters, cache, X, Y, keep_probability):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*np.matmul(dZ,cache['A' + str(number_of_layers)].T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dA = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)\n",
    "        dA = dA * cache[\"D\" + str(layer)]\n",
    "        dA = dA / keep_probability\n",
    "        dZ = dA * np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*np.matmul(dZ,cache['A' + str(layer-1)].T)\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dA = np.matmul(parameters['W2'].T,last_dZ)\n",
    "    dA = dA * cache[\"D1\"]\n",
    "    dA = dA / keep_probability\n",
    "    dZ1 = dA * np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\" and \"grads\"\n",
    "    # Update rule for each parameter\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        parameters['W'+str(layer)] = parameters['W'+str(layer)] - learning_rate*grads['dW'+str(layer)]\n",
    "        parameters['b'+str(layer)] = parameters['b'+str(layer)] - learning_rate*grads['db'+str(layer)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.heaviside(A2-0.5, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_train_accuracy(parameters):\n",
    "    predictions = predict(parameters, train_X)\n",
    "    return float((np.dot(train_Y,predictions.T) + np.dot(1-train_Y,1-predictions.T))/float(train_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(parameters):\n",
    "    predictions = predict(parameters, test_X)\n",
    "    return float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layer_dims, learning_rate, lambd, keep_probability, num_iterations, print_cost=False, print_graph = False, regularization = False, dropout = False):\n",
    "    costs=[]\n",
    "    train_accuracy_values = []\n",
    "    test_accuracy_values = []\n",
    "\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    print(\"\\nStart training:\")\n",
    "    for i in range(num_iterations):\n",
    "        if dropout:\n",
    "            A, cache = forward_propagation_with_dropout(X, parameters, keep_probability)\n",
    "        else:\n",
    "            A, cache = forward_propagation(X, parameters)\n",
    "        cost = 0\n",
    "        if regularization:\n",
    "            cost = compute_cost_with_regularization(A, Y, parameters, lambd)\n",
    "        else:\n",
    "            cost = compute_cost(A, Y)\n",
    "        costs.append(cost)\n",
    "        if print_cost == True:\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Cost after iteration {}: {:.2e}\".format(i+1, cost))\n",
    "\n",
    "        grads = {}\n",
    "        if regularization:\n",
    "            grads = backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n",
    "        elif dropout:\n",
    "            grads = backward_propagation_with_dropout(parameters, cache, X, Y, keep_probability)\n",
    "        else:\n",
    "            grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_graph:\n",
    "            train_accuracy_values.append(calculate_train_accuracy(parameters))\n",
    "            test_accuracy_values.append(calculate_test_accuracy(parameters))\n",
    "\n",
    "    \n",
    "    return (parameters, costs, train_accuracy_values, test_accuracy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a network with the following shapes of the weights and biases:\n",
      "W1:(256, 3136)\n",
      "b1:(256, 1)\n",
      "W2:(128, 256)\n",
      "b2:(128, 1)\n",
      "W3:(64, 128)\n",
      "b3:(64, 1)\n",
      "W4:(1, 64)\n",
      "b4:(1, 1)\n",
      "\n",
      "Start training:\n",
      "Cost after iteration 1: 6.93e-01\n",
      "Cost after iteration 2: 6.93e-01\n",
      "Cost after iteration 3: 6.93e-01\n",
      "Cost after iteration 4: 6.93e-01\n",
      "Cost after iteration 5: 6.93e-01\n",
      "Cost after iteration 6: 6.93e-01\n",
      "Cost after iteration 7: 6.93e-01\n",
      "Cost after iteration 8: 6.93e-01\n",
      "Cost after iteration 9: 6.93e-01\n",
      "Cost after iteration 10: 6.93e-01\n",
      "Cost after iteration 11: 6.93e-01\n",
      "Cost after iteration 12: 6.93e-01\n",
      "Cost after iteration 13: 6.93e-01\n",
      "Cost after iteration 14: 6.93e-01\n",
      "Cost after iteration 15: 6.93e-01\n",
      "Cost after iteration 16: 6.93e-01\n",
      "Cost after iteration 17: 6.92e-01\n",
      "Cost after iteration 18: 6.92e-01\n",
      "Cost after iteration 19: 6.92e-01\n",
      "Cost after iteration 20: 6.92e-01\n",
      "Cost after iteration 21: 6.92e-01\n",
      "Cost after iteration 22: 6.92e-01\n",
      "Cost after iteration 23: 6.92e-01\n",
      "Cost after iteration 24: 6.92e-01\n",
      "Cost after iteration 25: 6.92e-01\n",
      "Cost after iteration 26: 6.92e-01\n",
      "Cost after iteration 27: 6.92e-01\n",
      "Cost after iteration 28: 6.92e-01\n",
      "Cost after iteration 29: 6.92e-01\n",
      "Cost after iteration 30: 6.92e-01\n",
      "Cost after iteration 31: 6.92e-01\n",
      "Cost after iteration 32: 6.92e-01\n",
      "Cost after iteration 33: 6.92e-01\n",
      "Cost after iteration 34: 6.92e-01\n",
      "Cost after iteration 35: 6.92e-01\n",
      "Cost after iteration 36: 6.92e-01\n",
      "Cost after iteration 37: 6.91e-01\n",
      "Cost after iteration 38: 6.91e-01\n",
      "Cost after iteration 39: 6.91e-01\n",
      "Cost after iteration 40: 6.91e-01\n",
      "Cost after iteration 41: 6.91e-01\n",
      "Cost after iteration 42: 6.91e-01\n",
      "Cost after iteration 43: 6.91e-01\n",
      "Cost after iteration 44: 6.91e-01\n",
      "Cost after iteration 45: 6.91e-01\n",
      "Cost after iteration 46: 6.91e-01\n",
      "Cost after iteration 47: 6.91e-01\n",
      "Cost after iteration 48: 6.91e-01\n",
      "Cost after iteration 49: 6.91e-01\n",
      "Cost after iteration 50: 6.90e-01\n",
      "Cost after iteration 51: 6.90e-01\n",
      "Cost after iteration 52: 6.90e-01\n",
      "Cost after iteration 53: 6.90e-01\n",
      "Cost after iteration 54: 6.90e-01\n",
      "Cost after iteration 55: 6.90e-01\n",
      "Cost after iteration 56: 6.90e-01\n",
      "Cost after iteration 57: 6.90e-01\n",
      "Cost after iteration 58: 6.90e-01\n",
      "Cost after iteration 59: 6.89e-01\n",
      "Cost after iteration 60: 6.89e-01\n",
      "Cost after iteration 61: 6.89e-01\n",
      "Cost after iteration 62: 6.89e-01\n",
      "Cost after iteration 63: 6.89e-01\n",
      "Cost after iteration 64: 6.88e-01\n",
      "Cost after iteration 65: 6.88e-01\n",
      "Cost after iteration 66: 6.88e-01\n",
      "Cost after iteration 67: 6.87e-01\n",
      "Cost after iteration 68: 6.87e-01\n",
      "Cost after iteration 69: 6.87e-01\n",
      "Cost after iteration 70: 6.86e-01\n",
      "Cost after iteration 71: 6.86e-01\n",
      "Cost after iteration 72: 6.85e-01\n",
      "Cost after iteration 73: 6.85e-01\n",
      "Cost after iteration 74: 6.84e-01\n",
      "Cost after iteration 75: 6.84e-01\n",
      "Cost after iteration 76: 6.83e-01\n",
      "Cost after iteration 77: 6.82e-01\n",
      "Cost after iteration 78: 6.81e-01\n",
      "Cost after iteration 79: 6.80e-01\n",
      "Cost after iteration 80: 6.78e-01\n",
      "Cost after iteration 81: 6.77e-01\n",
      "Cost after iteration 82: 6.75e-01\n",
      "Cost after iteration 83: 6.73e-01\n",
      "Cost after iteration 84: 6.72e-01\n",
      "Cost after iteration 85: 6.69e-01\n",
      "Cost after iteration 86: 6.66e-01\n",
      "Cost after iteration 87: 6.63e-01\n",
      "Cost after iteration 88: 6.58e-01\n",
      "Cost after iteration 89: 6.54e-01\n",
      "Cost after iteration 90: 6.49e-01\n",
      "Cost after iteration 91: 6.45e-01\n",
      "Cost after iteration 92: 6.39e-01\n",
      "Cost after iteration 93: 6.32e-01\n",
      "Cost after iteration 94: 6.26e-01\n",
      "Cost after iteration 95: 6.17e-01\n",
      "Cost after iteration 96: 6.11e-01\n",
      "Cost after iteration 97: 6.04e-01\n",
      "Cost after iteration 98: 5.99e-01\n",
      "Cost after iteration 99: 5.95e-01\n",
      "Cost after iteration 100: 5.92e-01\n",
      "Cost after iteration 101: 5.90e-01\n",
      "Cost after iteration 102: 5.89e-01\n",
      "Cost after iteration 103: 5.88e-01\n",
      "Cost after iteration 104: 5.87e-01\n",
      "Cost after iteration 105: 5.81e-01\n",
      "Cost after iteration 106: 5.83e-01\n",
      "Cost after iteration 107: 5.80e-01\n",
      "Cost after iteration 108: 5.82e-01\n",
      "Cost after iteration 109: 5.81e-01\n",
      "Cost after iteration 110: 5.84e-01\n",
      "Cost after iteration 111: 5.80e-01\n",
      "Cost after iteration 112: 5.82e-01\n",
      "Cost after iteration 113: 5.80e-01\n",
      "Cost after iteration 114: 5.83e-01\n",
      "Cost after iteration 115: 5.81e-01\n",
      "Cost after iteration 116: 5.80e-01\n",
      "Cost after iteration 117: 5.79e-01\n",
      "Cost after iteration 118: 5.80e-01\n",
      "Cost after iteration 119: 5.78e-01\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [train_X.shape[0], 256, 128, 64, train_Y.shape[0]]\n",
    "learning_rate = 0.2\n",
    "lambd = 0.7\n",
    "keep_probability = 0.5\n",
    "number_of_iterations = 3000\n",
    "print_cost = True\n",
    "print_graph = True\n",
    "regularization = False\n",
    "dropout = True\n",
    "\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs, train_accuracy_values, test_accuracy_values = nn_model(\n",
    "    X=train_X, \n",
    "    Y=train_Y, \n",
    "    layer_dims=layer_dims, \n",
    "    learning_rate=learning_rate, \n",
    "    lambd=lambd,\n",
    "    keep_probability = keep_probability,\n",
    "    num_iterations = number_of_iterations, \n",
    "    print_cost=print_cost, \n",
    "    print_graph=print_graph, \n",
    "    regularization=regularization,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "title_string = \"alpha: {}; layers: {};\".format(str(learning_rate), str(layer_dims))\n",
    "if regularization:\n",
    "    title_string = title_string + \" lambda: {};\".format(str(lambd))\n",
    "if dropout:\n",
    "    title_string = title_string + \" keep probability: {};\".format(str(keep_probability))\n",
    "\n",
    "plt.title(title_string)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "if print_graph:\n",
    "    plt.plot(train_accuracy_values, label='train accuracy')\n",
    "    plt.plot(test_accuracy_values, label='test accuracy')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "    plt.title(\"train accuracy and test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
