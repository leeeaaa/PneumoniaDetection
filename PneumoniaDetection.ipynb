{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return train_X, train_Y, test_X, test_Y, val_X, val_Y\n",
    "def load_dataset():\n",
    "    scaled_train_dir = \"scaled_chest_xray/train/\"\n",
    "    scaled_test_dir = \"scaled_chest_xray/test/\"\n",
    "\n",
    "    scaled_train_pneu = os.listdir(os.path.join(scaled_train_dir, 'PNEUMONIA'))\n",
    "    scaled_train_normal = os.listdir(os.path.join(scaled_train_dir, 'NORMAL'))\n",
    "    scaled_test_pneu = os.listdir(os.path.join(scaled_test_dir, 'PNEUMONIA'))\n",
    "    scaled_test_normal = os.listdir(os.path.join(scaled_test_dir, 'NORMAL'))\n",
    "\n",
    "    scaled_train = [('PNEUMONIA/' + name, 1) for name in scaled_train_pneu] + [('NORMAL/' + name, 0) for name in scaled_train_normal]\n",
    "    scaled_test = [('PNEUMONIA/' + name, 1) for name in scaled_test_pneu] + [('NORMAL/' + name, 0) for name in scaled_test_normal]\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(scaled_train) \n",
    "    random.shuffle(scaled_test) \n",
    "\n",
    "    # data = np.array(list(Image.open(\"scaled_chest_xray/train/NORMAL/NORMAL-28501-0001.jpeg\").getdata()))\n",
    "    # print(len(data))\n",
    "    # print(data)\n",
    "    # print(type(data))\n",
    "    \n",
    "    test_X_list = [list(Image.open(scaled_test_dir + image_path).getdata().convert('L')) for image_path, i in scaled_test]\n",
    "    test_X_list = [[float(value)/255 for value in image_data] for image_data in test_X_list]\n",
    "\n",
    "    train_X_list = [list(Image.open(scaled_train_dir + image_path).getdata().convert('L')) for image_path, i in scaled_train]\n",
    "    train_X_list = [[float(value)/255 for value in image_data] for image_data in train_X_list]\n",
    "\n",
    "    train_X = np.array(train_X_list, dtype=float).T\n",
    "    test_X = np.array(test_X_list, dtype=float).T\n",
    "    train_Y = np.array([[float(i) for image_path, i in scaled_train]], dtype=float)\n",
    "    test_Y = np.array([[float(i) for image_path, i in scaled_test]], dtype=float)\n",
    "\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "#print(train_X[0:3])\n",
    "#print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    number_of_layers = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    print(\"Generating a network with the following shapes of the weights and biases:\")\n",
    "    for layer in range(1, number_of_layers):\n",
    "        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1])*0.01\n",
    "        parameters['b'  + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "        \n",
    "        print(\"{}:{}\".format('W' + str(layer), parameters['W' + str(layer)].shape))\n",
    "        print(\"{}:{}\".format('b' + str(layer), parameters['b' + str(layer)].shape))\n",
    "\n",
    "        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))\n",
    "        assert(parameters['b'  + str(layer)].shape == (layer_dims[layer], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    # And implement Forward Propagation to calculate A2 (probabilities)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 8 lines of code)\n",
    "    cache = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    lastA = X\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        Z = np.matmul(parameters['W'+ str(layer)], lastA) + parameters['b' + str(layer)]\n",
    "        A = relu(Z)\n",
    "        cache.update({'Z' + str(layer): Z})\n",
    "        cache.update({'A' + str(layer): A})\n",
    "        lastA = A \n",
    "    Z = np.matmul(parameters['W' + str(number_of_layers+1)], lastA) + parameters['b' + str(number_of_layers+1)]\n",
    "    A = sigmoid(Z)\n",
    "    cache.update({'Z' + str(number_of_layers+1): Z})\n",
    "    cache.update({'A' + str(number_of_layers+1): A})\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    cost = -(1/m)* float(np.dot(Y, np.log(A.T)) + np.dot(1-Y,np.log(1-A.T)))\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # This gives you the cross-entropy part of the cost\n",
    "    cross_entropy_cost = compute_cost(A, Y)\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 line)\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    sum_of_all_weights = 0.0\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        sum_of_all_weights = sum_of_all_weights + np.sum(np.square(parameters['W' + str(layer)]))\n",
    "\n",
    "    L2_regularization_cost = lambd/(2*m) * sum_of_all_weights\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    ### END CODER HERE ###\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*np.matmul(dZ,cache['A' + str(number_of_layers)].T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*np.matmul(dZ,cache['A' + str(layer-1)].T)\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(parameters, cache, X, Y, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*(np.matmul(dZ,cache['A' + str(number_of_layers)].T)+lambd*parameters['W' + str(number_of_layers+1)])\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*(np.matmul(dZ,cache['A' + str(layer-1)].T)+lambd*parameters['W' + str(layer)])\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)    #(1,1)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*(np.matmul(dZ1,X.T)+lambd*parameters['W1'])\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\" and \"grads\"\n",
    "    # Update rule for each parameter\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        parameters['W'+str(layer)] = parameters['W'+str(layer)] - learning_rate*grads['dW'+str(layer)]\n",
    "        parameters['b'+str(layer)] = parameters['b'+str(layer)] - learning_rate*grads['db'+str(layer)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.heaviside(A2-0.5, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_train_accuracy(parameters):\n",
    "    predictions = predict(parameters, train_X)\n",
    "    return float((np.dot(train_Y,predictions.T) + np.dot(1-train_Y,1-predictions.T))/float(train_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(parameters):\n",
    "    predictions = predict(parameters, test_X)\n",
    "    return float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layer_dims, learning_rate, lambd, num_iterations, print_cost=False, print_graph = False):\n",
    "    costs=[]\n",
    "    train_accuracy_values = []\n",
    "    test_accuracy_values = []\n",
    "\n",
    "    regularization = False\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    print(\"\\nStart training:\")\n",
    "    for i in range(num_iterations):\n",
    "        A, cache = forward_propagation(X, parameters)\n",
    "        cost = []\n",
    "        if regularization:\n",
    "            cost = compute_cost_with_regularization(A, Y, parameters, lambd)\n",
    "        else:\n",
    "            cost = compute_cost(A, Y)\n",
    "        costs.append(cost)\n",
    "        if print_cost == True:\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Cost after iteration {}: {:.2e}\".format(i+1, cost))\n",
    "\n",
    "        grads = {}\n",
    "        if regularization:\n",
    "            grads = backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n",
    "        else:\n",
    "            grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_graph:\n",
    "            train_accuracy_values.append(calculate_train_accuracy(parameters))\n",
    "            test_accuracy_values.append(calculate_test_accuracy(parameters))\n",
    "\n",
    "    \n",
    "    return (parameters, costs, train_accuracy_values, test_accuracy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a network with the following shapes of the weights and biases:\n",
      "W1:(200, 50176)\n",
      "b1:(200, 1)\n",
      "W2:(1, 200)\n",
      "b2:(1, 1)\n",
      "\n",
      "Start training:\n",
      "Cost after iteration 1: 6.98e-01\n",
      "Cost after iteration 2: 1.80e+00\n",
      "Cost after iteration 3: 7.60e-01\n",
      "Cost after iteration 4: 2.27e+00\n",
      "Cost after iteration 5: 5.30e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [85], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m print_graph \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Build a model with a n_h-dimensional hidden layer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m parameters, costs, train_accuracy_values, test_accuracy_values \u001b[39m=\u001b[39m nn_model(\n\u001b[0;32m     10\u001b[0m     train_X, train_Y, layer_dims, lambd, \n\u001b[0;32m     11\u001b[0m     learning_rate, num_iterations \u001b[39m=\u001b[39;49m number_of_iterations, \n\u001b[0;32m     12\u001b[0m     print_cost\u001b[39m=\u001b[39;49mprint_cost, print_graph\u001b[39m=\u001b[39;49mprint_graph\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m plt\u001b[39m.\u001b[39mplot(costs)\n\u001b[0;32m     16\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mcost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [83], line 29\u001b[0m, in \u001b[0;36mnn_model\u001b[1;34m(X, Y, layer_dims, learning_rate, lambd, num_iterations, print_cost, print_graph)\u001b[0m\n\u001b[0;32m     26\u001b[0m     parameters \u001b[39m=\u001b[39m update_parameters(parameters, grads, learning_rate)\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m print_graph:\n\u001b[1;32m---> 29\u001b[0m         train_accuracy_values\u001b[39m.\u001b[39mappend(calculate_train_accuracy(parameters))\n\u001b[0;32m     30\u001b[0m         test_accuracy_values\u001b[39m.\u001b[39mappend(calculate_test_accuracy(parameters))\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m (parameters, costs, train_accuracy_values, test_accuracy_values)\n",
      "Cell \u001b[1;32mIn [81], line 2\u001b[0m, in \u001b[0;36mcalculate_train_accuracy\u001b[1;34m(parameters)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_train_accuracy\u001b[39m(parameters):\n\u001b[1;32m----> 2\u001b[0m     predictions \u001b[39m=\u001b[39m predict(parameters, train_X)\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39m((np\u001b[39m.\u001b[39mdot(train_Y,predictions\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mtrain_Y,\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mpredictions\u001b[39m.\u001b[39mT))\u001b[39m/\u001b[39m\u001b[39mfloat\u001b[39m(train_Y\u001b[39m.\u001b[39msize)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn [80], line 15\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(parameters, X)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mUsing the learned parameters, predicts a class for each example in X\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mpredictions -- vector of predictions of our model (red: 0 / blue: 1)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m### START CODE HERE ### (≈ 2 lines of code)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m A2, cache \u001b[39m=\u001b[39m forward_propagation(X, parameters)\n\u001b[0;32m     16\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mheaviside(A2\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [74], line 19\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m     17\u001b[0m lastA \u001b[39m=\u001b[39m X\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, number_of_layers\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     Z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(parameters[\u001b[39m'\u001b[39;49m\u001b[39mW\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(layer)], lastA) \u001b[39m+\u001b[39m parameters[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer)]\n\u001b[0;32m     20\u001b[0m     A \u001b[39m=\u001b[39m relu(Z)\n\u001b[0;32m     21\u001b[0m     cache\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer): Z})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dims = [train_X.shape[0], 100, train_Y.shape[0]]\n",
    "learning_rate = 0.005\n",
    "lambd = 0.2\n",
    "number_of_iterations = 300\n",
    "print_cost = True\n",
    "print_graph = True\n",
    "\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs, train_accuracy_values, test_accuracy_values = nn_model(\n",
    "    train_X, train_Y, layer_dims, lambd, \n",
    "    learning_rate, num_iterations = number_of_iterations, \n",
    "    print_cost=print_cost, print_graph=print_graph\n",
    ")\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "plt.title(\"alpha: {}; lambda: {}; layers: {}\".format(str(learning_rate),str(lambd), str(layer_dims)))\n",
    "plt.show()\n",
    "\n",
    "if print_graph:\n",
    "    plt.plot(train_accuracy_values)\n",
    "    plt.plot(test_accuracy_values)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "    plt.title(\"train accuracy (blue); test accuracy (orange)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
