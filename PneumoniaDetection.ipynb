{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return train_X, train_Y, test_X, test_Y, val_X, val_Y\n",
    "def load_dataset():\n",
    "    scaled_train_dir = \"scaled_chest_xray/train/\"\n",
    "    scaled_test_dir = \"scaled_chest_xray/test/\"\n",
    "\n",
    "    scaled_train_pneu = os.listdir(os.path.join(scaled_train_dir, 'PNEUMONIA'))\n",
    "    scaled_train_normal = os.listdir(os.path.join(scaled_train_dir, 'NORMAL'))\n",
    "    scaled_test_pneu = os.listdir(os.path.join(scaled_test_dir, 'PNEUMONIA'))\n",
    "    scaled_test_normal = os.listdir(os.path.join(scaled_test_dir, 'NORMAL'))\n",
    "\n",
    "    scaled_train = [('PNEUMONIA/' + name, 1) for name in scaled_train_pneu] + [('NORMAL/' + name, 0) for name in scaled_train_normal]\n",
    "    scaled_test = [('PNEUMONIA/' + name, 1) for name in scaled_test_pneu] + [('NORMAL/' + name, 0) for name in scaled_test_normal]\n",
    "\n",
    "    random.shuffle(scaled_train) \n",
    "    random.shuffle(scaled_test) \n",
    "\n",
    "    # data = np.array(list(Image.open(\"scaled_chest_xray/train/NORMAL/NORMAL-28501-0001.jpeg\").getdata()))\n",
    "    # print(len(data))\n",
    "    # print(data)\n",
    "    # print(type(data))\n",
    "    \n",
    "    test_X_list = [list(Image.open(scaled_test_dir + image_path).getdata().convert('L')) for image_path, i in scaled_test]\n",
    "    test_X_list = [[float(value)/255 for value in image_data] for image_data in test_X_list]\n",
    "\n",
    "    train_X_list = [list(Image.open(scaled_train_dir + image_path).getdata().convert('L')) for image_path, i in scaled_train]\n",
    "    train_X_list = [[float(value)/255 for value in image_data] for image_data in train_X_list]\n",
    "\n",
    "    train_X = np.array(train_X_list, dtype=float).T\n",
    "    test_X = np.array(test_X_list, dtype=float).T\n",
    "    train_Y = np.array([[float(i) for image_path, i in scaled_train]], dtype=float)\n",
    "    test_Y = np.array([[float(i) for image_path, i in scaled_test]], dtype=float)\n",
    "\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.27843137 0.00392157 ... 0.00784314 0.00392157 0.04313725]\n",
      " [0.         0.39607843 0.         ... 0.         0.         0.04313725]\n",
      " [0.         0.36862745 0.         ... 0.         0.         0.04313725]\n",
      " ...\n",
      " [0.         0.03921569 0.00392157 ... 0.         0.         0.04313725]\n",
      " [0.01176471 0.         0.         ... 0.         0.         0.04313725]\n",
      " [0.         0.         0.         ... 0.         0.         0.04313725]]\n",
      "(50176, 5232)\n",
      "(1, 5232)\n",
      "(50176, 624)\n",
      "(1, 624)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "#print(train_X[0:3])\n",
    "#print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (parameters[\"W1\"].shape == (n_h, n_x))\n",
    "    assert (parameters[\"b1\"].shape == (n_h, 1))\n",
    "    assert (parameters[\"W2\"].shape == (n_y, n_h))\n",
    "    assert (parameters[\"b2\"].shape == (n_y, 1))\n",
    "     \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    # And implement Forward Propagation to calculate A2 (probabilities)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 8 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    #print(W1.shape)\n",
    "    b1 = parameters[\"b1\"]\n",
    "    #print(b1.shape)\n",
    "    W2 = parameters[\"W2\"]\n",
    "    #print(W2.shape)\n",
    "    b2 = parameters[\"b2\"]\n",
    "    #print(b2.shape)\n",
    "\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    #print(Z1.shape)\n",
    "    A1 = relu(Z1) ## !! CHOOSE AN ACTIVATION FUNCTION\n",
    "    #print(A1.shape)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    #print(Z2.shape)\n",
    "    A2 = sigmoid(Z2) ## !! CHOOSE AN ACTIVITAION FUNCTION\n",
    "    #print(A2.shape)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    # Values needed in the backpropagation are stored in \"cache\". This will be given as an input to the backpropagation\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    cost = -(1/m)* float(np.dot(Y, np.log(A2.T)) + np.dot(1-Y,np.log(1-A2.T)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    A1 = cache[\"A1\"]        #(4,3)\n",
    "    A2 = cache[\"A2\"]        #(1,3)\n",
    "    W2 = parameters[\"W2\"]   #(1,4)\n",
    "\n",
    "    dZ2 = A2 - Y                                        #(1,3)\n",
    "    dW2 = (1/m)*np.matmul(dZ2,A1.T)                     #(1,4)\n",
    "    db2 = (1/m)*np.sum(dZ2, axis=1, keepdims = True)    #(1,1)\n",
    "\n",
    "    #dZ1 = np.matmul(W2.T,dZ2)* (1-np.power(A1, 2))      #(4,3) # !!! CHOOSE CORRECT DERRIVATIVE FOR THE ACTIVATION FUNCTION IN THE 2. LAYER\n",
    "    dZ1 = np.matmul(W2.T,dZ2)* np.heaviside(A1, 1)      #(4,3)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)                      #(4,2)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)    #(4,1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\" and \"grads\"\n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 12 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, learning_rate, num_iterations = 10000, print_cost=False):\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    costs=[]\n",
    "\n",
    "    # Initialize parameters\n",
    "    # Loop (gradient descent)\n",
    "    # Print every 1000 th cost to console, e.g. using print(\"Cost after iteration {}: {:.2e}\".format(i, cost))\n",
    "    ### START CODE HERE ### (≈ 12 lines of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    #return parameters\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        costs.append(cost)\n",
    "        if print_cost == True:\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Cost after iteration {}: {:.2e}\".format(i+1, cost))\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Returns parameters learnt by the model. They can then be used to predict output\n",
    "    return (parameters, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.heaviside(A2-0.5, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 1: 6.71e-01\n",
      "Cost after iteration 2: 6.49e-01\n",
      "Cost after iteration 3: 6.29e-01\n",
      "Cost after iteration 4: 6.13e-01\n",
      "Cost after iteration 5: 6.00e-01\n",
      "Cost after iteration 6: 5.89e-01\n",
      "Cost after iteration 7: 5.82e-01\n",
      "Cost after iteration 8: 5.77e-01\n",
      "Cost after iteration 9: 5.73e-01\n",
      "Cost after iteration 10: 5.70e-01\n",
      "Cost after iteration 11: 5.68e-01\n",
      "Cost after iteration 12: 5.67e-01\n",
      "Cost after iteration 13: 5.66e-01\n",
      "Cost after iteration 14: 5.65e-01\n",
      "Cost after iteration 15: 5.65e-01\n",
      "Cost after iteration 16: 5.64e-01\n",
      "Cost after iteration 17: 5.64e-01\n",
      "Cost after iteration 18: 5.63e-01\n",
      "Cost after iteration 19: 5.63e-01\n",
      "Cost after iteration 20: 5.62e-01\n",
      "Cost after iteration 21: 5.61e-01\n",
      "Cost after iteration 22: 5.61e-01\n",
      "Cost after iteration 23: 5.60e-01\n",
      "Cost after iteration 24: 5.60e-01\n",
      "Cost after iteration 25: 5.59e-01\n",
      "Cost after iteration 26: 5.59e-01\n",
      "Cost after iteration 27: 5.58e-01\n",
      "Cost after iteration 28: 5.58e-01\n",
      "Cost after iteration 29: 5.57e-01\n",
      "Cost after iteration 30: 5.57e-01\n",
      "Cost after iteration 31: 5.56e-01\n",
      "Cost after iteration 32: 5.56e-01\n",
      "Cost after iteration 33: 5.55e-01\n",
      "Cost after iteration 34: 5.54e-01\n",
      "Cost after iteration 35: 5.54e-01\n",
      "Cost after iteration 36: 5.53e-01\n",
      "Cost after iteration 37: 5.53e-01\n",
      "Cost after iteration 38: 5.52e-01\n",
      "Cost after iteration 39: 5.51e-01\n",
      "Cost after iteration 40: 5.51e-01\n",
      "Cost after iteration 41: 5.50e-01\n",
      "Cost after iteration 42: 5.50e-01\n",
      "Cost after iteration 43: 5.49e-01\n",
      "Cost after iteration 44: 5.48e-01\n",
      "Cost after iteration 45: 5.48e-01\n",
      "Cost after iteration 46: 5.47e-01\n",
      "Cost after iteration 47: 5.46e-01\n",
      "Cost after iteration 48: 5.46e-01\n",
      "Cost after iteration 49: 5.45e-01\n",
      "Cost after iteration 50: 5.44e-01\n",
      "Cost after iteration 51: 5.44e-01\n",
      "Cost after iteration 52: 5.43e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [150], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.005\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Build a model with a n_h-dimensional hidden layer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m parameters, costs \u001b[39m=\u001b[39m nn_model(train_X, train_Y, n_h, learning_rate, num_iterations \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, print_cost\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      9\u001b[0m plt\u001b[39m.\u001b[39mplot(costs)\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mcost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [148], line 19\u001b[0m, in \u001b[0;36mnn_model\u001b[1;34m(X, Y, n_h, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCost after iteration \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{:.2e}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, cost))\n\u001b[1;32m---> 19\u001b[0m     grads \u001b[39m=\u001b[39m backward_propagation(parameters, cache, X, Y)\n\u001b[0;32m     20\u001b[0m     parameters \u001b[39m=\u001b[39m update_parameters(parameters, grads, learning_rate)\n\u001b[0;32m     22\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[39m# Returns parameters learnt by the model. They can then be used to predict output\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [146], line 31\u001b[0m, in \u001b[0;36mbackward_propagation\u001b[1;34m(parameters, cache, X, Y)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m#dZ1 = np.matmul(W2.T,dZ2)* (1-np.power(A1, 2))      #(4,3) # !!! CHOOSE CORRECT DERRIVATIVE FOR THE ACTIVATION FUNCTION IN THE 2. LAYER\u001b[39;00m\n\u001b[0;32m     30\u001b[0m dZ1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2\u001b[39m.\u001b[39mT,dZ2)\u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mheaviside(A1, \u001b[39m1\u001b[39m)      \u001b[39m#(4,3)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m dW1 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(dZ1,X\u001b[39m.\u001b[39;49mT)                      \u001b[39m#(4,2)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m db1 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39msum(dZ1, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)    \u001b[39m#(4,1)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_x = train_X.shape[0]\n",
    "n_y = train_Y.shape[0]\n",
    "n_h = 100 # Number of neurons per layer\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs = nn_model(train_X, train_Y, n_h, learning_rate, num_iterations = 600, print_cost=True)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (x1,000)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, train_X)\n",
    "print ('Accuracy: %d' % float((np.dot(train_Y,predictions.T) + np.dot(1-train_Y,1-predictions.T))/float(train_Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, test_X)\n",
    "print ('Accuracy: %d' % float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
