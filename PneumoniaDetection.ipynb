{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return train_X, train_Y, test_X, test_Y, val_X, val_Y\n",
    "def load_dataset():\n",
    "    scaled_train_dir = \"scaled_chest_xray/train/\"\n",
    "    scaled_test_dir = \"scaled_chest_xray/test/\"\n",
    "\n",
    "    scaled_train_pneu = os.listdir(os.path.join(scaled_train_dir, 'PNEUMONIA'))\n",
    "    scaled_train_normal = os.listdir(os.path.join(scaled_train_dir, 'NORMAL'))\n",
    "    scaled_test_pneu = os.listdir(os.path.join(scaled_test_dir, 'PNEUMONIA'))\n",
    "    scaled_test_normal = os.listdir(os.path.join(scaled_test_dir, 'NORMAL'))\n",
    "\n",
    "    scaled_train = [('PNEUMONIA/' + name, 1) for name in scaled_train_pneu] + [('NORMAL/' + name, 0) for name in scaled_train_normal]\n",
    "    scaled_test = [('PNEUMONIA/' + name, 1) for name in scaled_test_pneu] + [('NORMAL/' + name, 0) for name in scaled_test_normal]\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(scaled_train) \n",
    "    random.shuffle(scaled_test) \n",
    "\n",
    "    # data = np.array(list(Image.open(\"scaled_chest_xray/train/NORMAL/NORMAL-28501-0001.jpeg\").getdata()))\n",
    "    # print(len(data))\n",
    "    # print(data)\n",
    "    # print(type(data))\n",
    "    \n",
    "    test_X_list = [list(Image.open(scaled_test_dir + image_path).getdata().convert('L')) for image_path, i in scaled_test]\n",
    "    test_X_list = [[float(value)/255 for value in image_data] for image_data in test_X_list]\n",
    "\n",
    "    train_X_list = [list(Image.open(scaled_train_dir + image_path).getdata().convert('L')) for image_path, i in scaled_train]\n",
    "    train_X_list = [[float(value)/255 for value in image_data] for image_data in train_X_list]\n",
    "\n",
    "    train_X = np.array(train_X_list, dtype=float).T\n",
    "    test_X = np.array(test_X_list, dtype=float).T\n",
    "    train_Y = np.array([[float(i) for image_path, i in scaled_train]], dtype=float)\n",
    "    test_Y = np.array([[float(i) for image_path, i in scaled_test]], dtype=float)\n",
    "\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.79607843 0.01960784 ... 0.1254902  0.68235294 0.        ]\n",
      " [0.         0.78431373 0.03529412 ... 0.1254902  0.70980392 0.        ]\n",
      " [0.         0.78039216 0.03137255 ... 0.1254902  0.69019608 0.00784314]\n",
      " ...\n",
      " [0.         0.05490196 0.06666667 ... 0.11372549 0.45098039 0.        ]\n",
      " [0.         0.08235294 0.06666667 ... 0.10588235 0.38823529 0.        ]\n",
      " [0.         0.10196078 0.06666667 ... 0.09803922 0.38431373 0.        ]]\n",
      "[[1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      "  0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      "  0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0.\n",
      "  0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      "  1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      "  1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      "  1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      "  0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      "  1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      "  1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      "  0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      "  1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      "  0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      "  0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      "  1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      "  1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      "  0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      "  1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1.\n",
      "  1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0.]]\n",
      "(50176, 5232)\n",
      "(1, 5232)\n",
      "(50176, 624)\n",
      "(1, 624)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "#print(train_X[0:3])\n",
    "#print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    number_of_layers = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for layer in range(1, number_of_layers):\n",
    "        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1])*0.01\n",
    "        parameters['b'  + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "        \n",
    "        print(\"{}:{}\".format('W' + str(layer), parameters['W' + str(layer)].shape))\n",
    "        print(\"{}:{}\".format('b' + str(layer), parameters['b' + str(layer)].shape))\n",
    "\n",
    "        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))\n",
    "        assert(parameters['b'  + str(layer)].shape == (layer_dims[layer], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    # And implement Forward Propagation to calculate A2 (probabilities)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 8 lines of code)\n",
    "    cache = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    lastA = X\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        Z = np.matmul(parameters['W'+ str(layer)], lastA) + parameters['b' + str(layer)]\n",
    "        A = relu(Z)\n",
    "        cache.update({'Z' + str(layer): Z})\n",
    "        cache.update({'A' + str(layer): A})\n",
    "        lastA = A \n",
    "    Z = np.matmul(parameters['W' + str(number_of_layers+1)], lastA) + parameters['b' + str(number_of_layers+1)]\n",
    "    print(Z)\n",
    "    A = sigmoid(Z)\n",
    "    cache.update({'Z' + str(number_of_layers+1): Z})\n",
    "    cache.update({'A' + str(number_of_layers+1): A})\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    #print(A)\n",
    "    cost = -(1/m)* float(np.dot(Y, np.log(A.T)) + np.dot(1-Y,np.log(1-A.T)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # This gives you the cross-entropy part of the cost\n",
    "    cross_entropy_cost = compute_cost(A, Y)\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 line)\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    sum_of_all_weights = 0.0\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        sum_of_all_weights = sum_of_all_weights + np.sum(np.square(parameters['W' + str(layer)]))\n",
    "\n",
    "    L2_regularization_cost = lambd/(2*m) * sum_of_all_weights\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    ### END CODER HERE ###\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*np.matmul(dZ,cache['A' + str(number_of_layers)].T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*np.matmul(dZ,cache['A' + str(layer-1)].T)\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(parameters, cache, X, Y, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*(np.matmul(dZ,cache['A' + str(number_of_layers)].T)+lambd*parameters['W' + str(number_of_layers+1)])\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*(np.matmul(dZ,cache['A' + str(layer-1)].T)+lambd*parameters['W' + str(layer)])\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)    #(1,1)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*(np.matmul(dZ1,X.T)+lambd*parameters['W1'])\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\" and \"grads\"\n",
    "    # Update rule for each parameter\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        parameters['W'+str(layer)] = parameters['W'+str(layer)] - learning_rate*grads['dW'+str(layer)]\n",
    "        parameters['b'+str(layer)] = parameters['b'+str(layer)] - learning_rate*grads['db'+str(layer)]\n",
    "    \n",
    "    # W1 = parameters[\"W1\"]\n",
    "    # b1 = parameters[\"b1\"]\n",
    "    # W2 = parameters[\"W2\"]\n",
    "    # b2 = parameters[\"b2\"]\n",
    "\n",
    "    # dW1 = grads[\"dW1\"]\n",
    "    # db1 = grads[\"db1\"]\n",
    "    # dW2 = grads[\"dW2\"]\n",
    "    # db2 = grads[\"db2\"]\n",
    "\n",
    "    # W1 = W1 - learning_rate*dW1\n",
    "    # b1 = b1 - learning_rate*db1\n",
    "    # W2 = W2 - learning_rate*dW2\n",
    "    # b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    # parameters = {\"W1\": W1,\n",
    "    #               \"b1\": b1,\n",
    "    #               \"W2\": W2,\n",
    "    #               \"b2\": b2}\n",
    "    \n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layer_dims, learning_rate, lambd, num_iterations = 10000, print_cost=False):\n",
    "    costs=[]\n",
    "    regularization = False\n",
    "    # Initialize parameters\n",
    "    # Loop (gradient descent)\n",
    "    # Print every 1000 th cost to console, e.g. using print(\"Cost after iteration {}: {:.2e}\".format(i, cost))\n",
    "    ### START CODE HERE ### (≈ 12 lines of code)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    #return parameters\n",
    "    for i in range(num_iterations):\n",
    "        A, cache = forward_propagation(X, parameters)\n",
    "        cost = []\n",
    "        if regularization:\n",
    "            cost = compute_cost_with_regularization(A, Y, parameters, lambd)\n",
    "        else:\n",
    "            cost = compute_cost(A, Y)\n",
    "        costs.append(cost)\n",
    "        if print_cost == True:\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Cost after iteration {}: {:.2e}\".format(i+1, cost))\n",
    "\n",
    "        grads = {}\n",
    "        if regularization:\n",
    "            grads = backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n",
    "        else:\n",
    "            grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Returns parameters learnt by the model. They can then be used to predict output\n",
    "    return (parameters, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.heaviside(A2-0.5, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:(100, 50176)\n",
      "b1:(100, 1)\n",
      "W2:(1, 100)\n",
      "b2:(1, 1)\n",
      "[[0.10503503 0.1045919  0.11057855 ... 0.08506437 0.09809489 0.10757789]]\n",
      "Cost after iteration 1: 6.71e-01\n",
      "[[3.14518017 3.18469824 3.2049382  ... 2.67046369 3.06805726 3.14217366]]\n",
      "Cost after iteration 2: 8.22e-01\n",
      "[[0.09481568 0.21971688 0.17749168 ... 0.15611727 0.09259702 0.13134133]]\n",
      "Cost after iteration 3: 6.50e-01\n",
      "[[2.94172096 3.15128274 3.1095065  ... 2.60495712 2.87254717 2.99171069]]\n",
      "Cost after iteration 4: 7.85e-01\n",
      "[[0.01722481 0.25127632 0.16303577 ... 0.15356788 0.04306498 0.08420284]]\n",
      "Cost after iteration 5: 6.47e-01\n",
      "[[2.90028095 3.27416497 3.17500394 ... 2.67204707 2.83711263 3.00144829]]\n",
      "Cost after iteration 6: 7.80e-01\n",
      "[[-0.02208058  0.3298368   0.19034298 ...  0.18948104  0.0187446\n",
      "   0.07491302]]\n",
      "Cost after iteration 7: 6.35e-01\n",
      "[[2.77130623 3.30809044 3.15081902 ... 2.66520215 2.71620408 2.92410922]]\n",
      "Cost after iteration 8: 7.58e-01\n",
      "[[-0.07571886  0.38632834  0.19664796 ...  0.20600538 -0.02733253\n",
      "   0.04910006]]\n",
      "Cost after iteration 9: 6.28e-01\n",
      "[[2.68366639 3.37969614 3.16636014 ... 2.69106147 2.6357692  2.88739216]]\n",
      "Cost after iteration 10: 7.44e-01\n",
      "[[-0.11773243  0.45540614  0.21382444 ...  0.2322071  -0.07152443\n",
      "   0.03466807]]\n",
      "Cost after iteration 11: 6.19e-01\n",
      "[[2.57680985 3.42877807 3.16011689 ... 2.69893384 2.53581671 2.83071322]]\n",
      "Cost after iteration 12: 7.26e-01\n",
      "[[-0.16225951  0.52597885  0.22694646 ...  0.25419641 -0.11879845\n",
      "   0.01893635]]\n",
      "Cost after iteration 13: 6.10e-01\n",
      "[[2.47912471 3.48364464 3.16087388 ... 2.712536   2.4441389  2.78232172]]\n",
      "Cost after iteration 14: 7.10e-01\n",
      "[[-0.2063354   0.59642471  0.24359594 ...  0.28158724 -0.16269329\n",
      "   0.00534405]]\n",
      "Cost after iteration 15: 6.01e-01\n",
      "[[2.37712407 3.53102454 3.15524078 ... 2.72083336 2.34800149 2.72903763]]\n",
      "Cost after iteration 16: 6.93e-01\n",
      "[[-0.25025535  0.66522772  0.26036345 ...  0.30871517 -0.20577626\n",
      "  -0.00877301]]\n",
      "Cost after iteration 17: 5.93e-01\n",
      "[[2.27646675 3.57653065 3.14894126 ... 2.72854201 2.25300992 2.67648682]]\n",
      "Cost after iteration 18: 6.77e-01\n",
      "[[-0.29091714  0.73393862  0.27815129 ...  0.33770807 -0.245261\n",
      "  -0.02269613]]\n",
      "Cost after iteration 19: 5.84e-01\n",
      "[[2.17464034 3.61774905 3.13950801 ... 2.73362976 2.15674267 2.62223363]]\n",
      "Cost after iteration 20: 6.61e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [113], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m number_of_iterations \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Build a model with a n_h-dimensional hidden layer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m parameters, costs \u001b[39m=\u001b[39m nn_model(train_X, train_Y, layer_dims, lambd, learning_rate, num_iterations \u001b[39m=\u001b[39;49m number_of_iterations, print_cost\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      9\u001b[0m plt\u001b[39m.\u001b[39mplot(costs)\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mcost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [111], line 26\u001b[0m, in \u001b[0;36mnn_model\u001b[1;34m(X, Y, layer_dims, learning_rate, lambd, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     24\u001b[0m         grads \u001b[39m=\u001b[39m backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n\u001b[0;32m     25\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m         grads \u001b[39m=\u001b[39m backward_propagation(parameters, cache, X, Y)\n\u001b[0;32m     27\u001b[0m     parameters \u001b[39m=\u001b[39m update_parameters(parameters, grads, learning_rate)\n\u001b[0;32m     28\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[39m# Returns parameters learnt by the model. They can then be used to predict output\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [108], line 40\u001b[0m, in \u001b[0;36mbackward_propagation\u001b[1;34m(parameters, cache, X, Y)\u001b[0m\n\u001b[0;32m     37\u001b[0m     last_dZ \u001b[39m=\u001b[39m dZ \n\u001b[0;32m     39\u001b[0m dZ1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(parameters[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mT,last_dZ)\u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mheaviside(cache[\u001b[39m'\u001b[39m\u001b[39mA1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m dW1 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(dZ1,X\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m     41\u001b[0m db1 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39msum(dZ1, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m grads\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mdW1\u001b[39m\u001b[39m'\u001b[39m: dW1})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dims = [train_X.shape[0], 100,train_Y.shape[0]]\n",
    "learning_rate = 0.005\n",
    "lambd = 0.2\n",
    "number_of_iterations = 600\n",
    "\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs = nn_model(train_X, train_Y, layer_dims, lambd, learning_rate, num_iterations = number_of_iterations, print_cost=True)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "plt.title(\"alpha: {}; lambda: {}; layers: {}\".format(str(learning_rate),str(lambd), str(layer_dims)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.97421204  4.37614562 -1.709526   ...  6.12680574  2.86820416\n",
      "   4.45814039]]\n",
      "Accuracy: 92%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, train_X)\n",
    "print ('Accuracy: %d' % float((np.dot(train_Y,predictions.T) + np.dot(1-train_Y,1-predictions.T))/float(train_Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.90446641e-01  2.89878590e+00  1.30834878e-02 -9.42141429e-01\n",
      "   3.72173790e+00 -1.88474823e-01  2.10689003e+00  1.80423592e+00\n",
      "   7.01022540e-02  2.60713790e+00 -2.33402221e-01  4.60745346e-01\n",
      "   3.11786082e+00  4.89063452e+00  1.77590822e+00  2.77637214e+00\n",
      "   1.20700822e+00  3.36644947e+00  3.88069316e-01  3.96091937e+00\n",
      "   2.50724040e+00  6.47639775e-01  2.43277749e+00  1.45653676e+00\n",
      "   1.31400629e+00  2.27692867e+00 -1.35523440e+00  3.09096632e-01\n",
      "   3.61478693e+00  8.04580358e-01 -2.69985230e-01  2.81345164e+00\n",
      "   4.08682136e+00  2.90429095e+00  2.04105225e+00  3.92847251e+00\n",
      "  -1.92563916e-01  7.82747996e-01  2.03635226e+00 -4.53384975e-01\n",
      "  -1.58618715e-01 -3.60861623e-01 -9.55404157e-02  3.00263787e+00\n",
      "   1.45508188e+00 -7.44494112e-01  4.03141684e+00 -1.31229274e+00\n",
      "   3.04574901e+00  2.26062873e+00 -7.72806456e-01 -1.21747801e+00\n",
      "  -1.64314566e+00  5.04734366e+00  3.30109671e+00  3.02885382e+00\n",
      "  -1.91507086e+00  3.16262820e+00  1.29007903e+00  2.99058097e+00\n",
      "  -4.64802989e-01 -3.40574091e-01  3.48709342e-01  2.26262883e-01\n",
      "   5.57470335e-01  4.12254885e-01  4.12321818e+00 -7.02592910e-01\n",
      "   1.43633776e+00  2.51206972e+00  3.51192701e+00  4.01338593e+00\n",
      "   2.82042698e+00  7.69009127e-01  1.94174590e+00  4.25649276e+00\n",
      "   2.13814110e+00  1.55304272e+00  3.66018348e+00  3.86892132e-01\n",
      "   3.18629508e+00  2.28344061e+00  3.68414518e+00 -1.64246286e-01\n",
      "   3.05626382e+00  4.56218964e+00 -1.14822988e+00  2.84542634e+00\n",
      "  -9.12472443e-01  2.08538878e+00  4.03524139e+00  1.45983770e+00\n",
      "   4.13902471e+00 -3.39256967e-01 -1.38492984e-01 -1.15128084e-01\n",
      "   1.76877176e+00  4.67526927e-01  3.11650728e+00 -1.55732309e+00\n",
      "  -5.23872703e-01  5.20859400e+00 -6.10515771e-01  3.14859353e+00\n",
      "   4.88448462e+00  1.85461134e+00  1.96098377e+00 -1.60959942e+00\n",
      "   3.71119553e+00  1.20527742e+00  4.97507223e+00  3.55396651e+00\n",
      "   1.79489637e-01  2.46082282e+00  1.68080156e+00 -6.21100236e-01\n",
      "  -3.35467045e-01  3.06418306e+00  2.51061548e-02  7.11060323e-01\n",
      "   5.93256125e-01  6.48136322e-02  1.44785671e+00  3.11664670e+00\n",
      "  -5.73444935e-01 -4.08942871e-01  2.34362524e+00 -1.48654926e+00\n",
      "   3.62467163e+00  2.63237256e-01  1.57692807e+00  2.23031138e+00\n",
      "   1.26444681e+00 -1.45364619e-01  3.69835065e+00  3.86122342e+00\n",
      "  -7.58022438e-01  4.21013427e+00  5.11640911e+00  4.76953876e+00\n",
      "   1.74047901e+00  3.94582415e+00  3.09172969e+00  3.90464145e+00\n",
      "   4.44143925e+00  2.16569236e+00  1.34723020e+00  6.75217689e-01\n",
      "   2.71157108e+00  1.21538190e+00  2.88114873e+00 -4.05384218e-01\n",
      "   4.67372839e+00  5.66739164e-02  3.64514624e+00  1.88530808e+00\n",
      "   2.39459999e+00  1.89427902e+00  2.44928305e+00  1.55490507e+00\n",
      "  -2.33959380e-01 -5.03926079e-01  3.76946202e+00  2.15383782e+00\n",
      "  -1.29133844e-01 -6.75911224e-01 -2.19465509e-01  8.07219952e-02\n",
      "   5.44361746e-01  6.34149549e-01  1.59786464e+00  1.05022614e+00\n",
      "  -3.59675526e-01  2.19527955e+00  1.52323124e+00  3.64989181e+00\n",
      "   1.29039420e-01  4.17478466e+00  1.02427261e+00  1.94622486e+00\n",
      "   5.02616169e-01  2.17013483e+00  8.19693665e-01  2.75862840e+00\n",
      "  -3.75439782e-01 -5.60876089e-01  3.45164398e+00  4.43684212e+00\n",
      "   3.78874834e+00  1.67622818e+00  4.23854413e+00 -7.88504928e-01\n",
      "  -7.72854557e-01  2.27440520e+00 -1.00217145e+00  3.90649539e-01\n",
      "   9.99721302e-01  9.09531905e-02  3.84680976e+00  2.70272603e+00\n",
      "  -3.19491221e-01  1.12367925e+00  2.62081043e-01  4.68506658e+00\n",
      "   6.05651842e-01  2.38389154e+00  3.57273802e-01  1.74429033e+00\n",
      "   2.24587436e+00  3.19730149e+00  3.06349201e+00  4.09152096e-02\n",
      "  -4.07859392e-01  3.21370030e+00  1.75786892e+00  3.77230080e+00\n",
      "   1.51832469e+00  1.05084299e+00  4.88859264e+00  1.13139579e+00\n",
      "   2.58537625e+00  4.50599029e+00  1.53323680e+00 -1.42822240e-01\n",
      "   5.38730424e-01 -4.21338556e-01  8.09947927e-01  2.54907048e-01\n",
      "   1.83151942e+00  1.94693534e+00  1.85338436e+00 -9.58599832e-01\n",
      "   2.47482982e+00 -1.13414743e-02  3.36625592e+00  1.10724110e+00\n",
      "   5.09932128e+00  9.45463960e-01  5.55268948e-01  4.64082265e+00\n",
      "   4.93302523e-01  4.30320515e+00  5.47401856e-01  3.16330373e+00\n",
      "   2.25195162e+00  3.79110357e+00  1.76305449e+00  3.38804222e+00\n",
      "   4.67457907e+00  3.39330899e-01  1.22457327e+00  2.50724040e+00\n",
      "  -6.68928578e-02 -2.66625732e-01  2.25947209e+00 -8.67779503e-01\n",
      "  -1.41334294e+00  8.55799611e-01 -1.53489198e+00  2.71616652e+00\n",
      "  -2.02935730e-01 -3.42754499e-01 -1.26162514e+00 -7.49628962e-01\n",
      "   1.06299532e+00  2.83373586e+00  2.18147667e+00  3.11484731e+00\n",
      "   2.57294696e+00  2.67515678e+00  2.60152270e+00 -6.36066699e-01\n",
      "  -8.24432875e-01 -1.02336703e+00  2.77161358e-01  1.03108192e+00\n",
      "   1.12348492e-01  2.90740922e+00  2.79938036e+00  3.41591745e+00\n",
      "  -3.21621468e-02  1.92947139e+00  2.03196987e+00  1.41437141e+00\n",
      "  -3.12413143e-01  4.53635151e+00  3.72151348e+00  6.25236762e-01\n",
      "   6.28696969e+00  2.91799873e+00  1.57023350e+00  2.54741800e+00\n",
      "   4.39901177e+00  1.38391995e+00  5.19054843e+00 -5.03169832e-01\n",
      "   4.35240870e+00  2.39860788e+00 -3.97071391e-01  1.89423602e+00\n",
      "  -6.68247093e-01 -8.81784314e-01  1.89232948e+00  4.20262009e+00\n",
      "   5.30944044e+00  1.24748026e+00 -4.68062110e-01  4.40094822e-01\n",
      "   2.58305902e+00  4.30152781e+00  1.80278136e+00 -1.30765135e+00\n",
      "   4.75995555e+00  2.51853852e+00  1.67019330e+00 -9.91984829e-01\n",
      "   3.66557628e+00  2.16492111e+00  1.81186380e-01  3.06033807e+00\n",
      "   2.07355917e-01  1.72601603e+00  3.42379921e+00  1.14194903e-01\n",
      "   5.01723418e-01  3.42607748e-01 -6.78181413e-01  5.40923339e-01\n",
      "  -4.65056801e-01  3.07326478e+00  3.45300519e+00  2.55239134e-01\n",
      "   7.53683429e-01 -5.94587372e-01 -7.71708537e-02  1.18862336e+00\n",
      "   2.23946669e+00  3.47091059e-01  3.35502212e+00  1.53855382e-01\n",
      "  -8.84568304e-01  3.02450250e+00  2.42437489e-01  1.07970298e+00\n",
      "  -7.96018552e-01  8.59726853e-01 -6.10780041e-01 -7.44534809e-01\n",
      "   2.05578904e-01  2.53897770e+00 -1.12098081e+00  3.87765721e+00\n",
      "   3.57331121e+00  6.87570461e+00  7.38467528e-02 -1.17114820e+00\n",
      "   1.03878786e+00 -2.01129732e-01  2.28082107e+00  4.29807957e+00\n",
      "   1.18364298e+00  3.36523405e+00 -1.42280307e+00 -5.23592817e-01\n",
      "   4.02909181e+00 -4.64667760e-01  3.51842586e+00 -1.08477137e-01\n",
      "   2.07692371e+00  8.21990630e-01  7.83431456e-01  2.09752872e+00\n",
      "   2.66287644e+00  3.78341489e+00  3.54172691e+00  3.47753268e+00\n",
      "   1.06963568e+00 -6.93779126e-01  1.13905795e+00  2.45348144e+00\n",
      "  -1.12717981e+00  3.26539834e+00  1.14876951e+00  1.04460805e+00\n",
      "   1.54367520e+00  5.29464698e+00  3.90250562e+00  2.28464392e-01\n",
      "  -1.34494000e-02  3.95287706e+00  1.74215338e+00  3.92217373e+00\n",
      "   3.35252048e+00  4.21198565e+00  2.09118293e+00  1.45701017e+00\n",
      "   1.37041953e+00  4.90785015e+00  8.39087608e-01  3.90299147e+00\n",
      "   1.09337198e+00  1.72934599e+00  3.05547829e-01  9.94866090e-01\n",
      "   4.96781525e+00  3.80451101e+00  2.98726292e+00  9.58936485e-01\n",
      "   1.80420854e+00  4.15943960e+00  4.23472409e+00  2.31361169e+00\n",
      "   5.40296851e+00  1.40684060e+00  9.80492785e-01  2.70741442e+00\n",
      "   3.55765962e+00  3.55235766e+00  2.92212246e+00  7.06780996e-01\n",
      "   3.55694225e+00  2.28714389e+00  1.39017233e+00  9.31703870e-01\n",
      "   3.36033261e+00 -4.12282544e-01  4.34152316e+00 -1.64426644e+00\n",
      "   2.43705922e+00 -2.31221781e-01  1.23058316e+00  5.13763005e+00\n",
      "  -1.47085121e+00 -9.59296520e-01 -5.74938985e-01 -8.57976042e-01\n",
      "   1.16892887e+01  4.22949617e+00  5.00498675e+00  3.23306463e-01\n",
      "   4.10174995e+00  5.02666589e+00  5.53859446e+00 -3.86433717e-01\n",
      "   2.64970129e+00  3.99819652e-01  1.95423134e-01  2.12968973e+00\n",
      "   2.90498413e+00  2.41075914e+00  2.24177509e+00 -1.23162025e+00\n",
      "   3.79496041e+00  2.73416877e+00 -1.42311018e+00  2.53552745e+00\n",
      "   1.31852316e+00  3.84403395e+00  9.18541801e-01  1.20351433e+00\n",
      "   3.95508626e-01  4.39698646e+00  5.11640911e+00  2.67133985e+00\n",
      "   6.76133595e-02  5.07565007e-01  6.68537161e-01  1.83468969e-01\n",
      "  -7.22055712e-01  2.81375579e+00  2.15352253e+00  2.82354392e+00\n",
      "   3.45915104e+00  1.59487736e+00  1.64964498e+00  4.92924206e+00\n",
      "   4.05966834e-01  2.20720147e-01  7.89086886e-01  1.38650479e+00\n",
      "  -9.44533579e-01 -7.20952887e-01  3.03804188e+00  3.95839384e+00\n",
      "   2.17289219e+00 -1.90268365e-01  3.42264442e+00  3.63124250e-01\n",
      "  -2.77767988e-01 -4.57341205e-01  2.54965161e-01  1.11855317e+00\n",
      "   2.41075914e+00 -1.57648600e+00  5.05641433e+00 -4.23686682e-01\n",
      "   4.76563774e+00  2.66204973e+00  3.57034234e-02  4.32109350e-01\n",
      "   4.38611958e+00 -7.14235345e-01  2.63565589e+00  3.34939241e+00\n",
      "  -1.23245318e+00 -9.67606007e-01  5.06677460e-01  1.00933018e+00\n",
      "   2.44668984e+00  7.44767390e-01  4.32094423e+00 -1.66240487e+00\n",
      "   1.33513126e+00  4.84751520e+00  2.37710012e+00  1.63057917e+00\n",
      "   2.77675229e+00 -8.72991518e-01  5.06864095e+00 -1.26407596e-01\n",
      "  -2.17358871e-01 -4.48962611e-01 -1.04549936e+00  1.18961980e+00\n",
      "  -4.67989011e-01 -4.85679204e-01 -4.25392243e-01 -9.48769405e-01\n",
      "  -6.51554919e-01  3.19648706e+00  1.33787882e+00 -9.47981395e-01\n",
      "   1.49954436e+00 -4.74953810e-02  2.36849878e+00  4.03647988e+00\n",
      "  -8.86908543e-01 -1.26266367e-01  9.41977789e-02  2.67042765e+00\n",
      "   4.62590445e+00  2.77173369e-01  4.17122258e-01  8.44515356e-01\n",
      "   4.65098633e+00  2.98271635e+00  1.56210415e+00  6.33769815e+00\n",
      "   4.66593788e+00  2.39942922e+00 -5.91862901e-01  2.46070686e+00\n",
      "   6.22709667e+00 -1.06192529e-01 -7.61983917e-02  2.28500621e+00\n",
      "   1.43683618e+00  1.39760105e+00 -1.67006799e-01 -1.47185437e+00\n",
      "   3.53456025e+00  3.05155252e+00  4.47167138e+00  3.44706669e+00\n",
      "   6.27391510e-01  4.04217467e-01  2.65567646e+00  3.17817607e+00\n",
      "   6.90498473e-01 -1.60945439e-01 -6.68437888e-01 -7.74574050e-01\n",
      "   9.69776055e-01  5.17164182e+00  4.47399757e+00  1.49528047e+00\n",
      "   2.47240232e+00  5.18656774e-01  2.52422975e+00  2.89892092e+00\n",
      "   4.01880045e+00  1.57934486e+00 -2.48387895e-01  3.11798390e-03\n",
      "  -1.88083316e-01 -1.50787004e+00  5.27174208e+00  8.97118042e-01\n",
      "   1.72766072e+00  2.45223967e+00 -8.72820273e-01  3.99317962e+00\n",
      "   2.14706905e+00 -5.21627802e-01 -1.37653429e+00  4.31061829e+00\n",
      "   2.76671555e+00 -4.72987114e-01  8.04452302e-02 -8.37003825e-01\n",
      "   2.86666371e+00  2.44532464e+00 -1.01734127e-01  3.57982714e+00\n",
      "   4.84460582e+00  4.77658786e+00  3.32221573e+00  1.22994783e-01\n",
      "  -1.07666218e+00 -3.23453635e-02  1.06092098e-01  1.37623182e+00\n",
      "  -4.05816257e-01  3.15215671e+00  1.54033252e+00  4.31969948e+00\n",
      "   3.13987281e+00 -7.86785075e-01  3.47503263e+00 -4.35218732e-01\n",
      "   6.42585642e-01  3.49689022e+00  3.56682934e+00  4.19522982e-01]]\n",
      "Accuracy: 83%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, test_X)\n",
    "print ('Accuracy: %d' % float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
