{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "NORMAL_OUTPUT_VALUE = 0\n",
    "PNEUMONIA_OUTPUT_VALUE = 1\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x - A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s - sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x - A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s - relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Collect all the paths of the scaled images and load them into the program\n",
    "    as numpy arrays.\n",
    "\n",
    "    Hint: The original images from https://www.kaggle.com/datasets/tolgadincer/labeled-chest-xray-images\n",
    "    have been downscaled with the sperate python script 'PythonImageScaler.py'\n",
    "\n",
    "    Returns:\n",
    "    train_X - All training images\n",
    "    train_Y - All true outputs of all training images\n",
    "    test_X - All test images\n",
    "    test_Y - All true outputs of all test images\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the root directories of the train and test sets.\n",
    "    scaled_train_dir = \"scaled_chest_xray/train/\"\n",
    "    scaled_test_dir = \"scaled_chest_xray/test/\"\n",
    "\n",
    "    # Define the root directories of the normal and pneumonia \n",
    "    # test and trainsets.\n",
    "    scaled_train_pneu = os.listdir(os.path.join(scaled_train_dir, 'PNEUMONIA'))\n",
    "    scaled_train_normal = os.listdir(os.path.join(scaled_train_dir, 'NORMAL'))\n",
    "    scaled_test_pneu = os.listdir(os.path.join(scaled_test_dir, 'PNEUMONIA'))\n",
    "    scaled_test_normal = os.listdir(os.path.join(scaled_test_dir, 'NORMAL'))\n",
    "\n",
    "    # Collect all paths from the training images and test images\n",
    "    # and combine these with the true output value (either \n",
    "    # NORMAL_OUTPUT_VALUE or PNEUMONIA_OUTPUT_VALUE)\n",
    "    train_set_image_paths = [('PNEUMONIA/' + name, PNEUMONIA_OUTPUT_VALUE) for name in scaled_train_pneu] + [('NORMAL/' + name, NORMAL_OUTPUT_VALUE) for name in scaled_train_normal]\n",
    "    test_set_image_paths = [('PNEUMONIA/' + name, PNEUMONIA_OUTPUT_VALUE) for name in scaled_test_pneu] + [('NORMAL/' + name, NORMAL_OUTPUT_VALUE) for name in scaled_test_normal]\n",
    "\n",
    "    # Shuffle the lists\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(train_set_image_paths) \n",
    "    random.shuffle(test_set_image_paths) \n",
    "\n",
    "    # Open all image paths and read all the grayscale data from every\n",
    "    # grayscale image. \n",
    "    # \".convert('L')\" makes sure that the image is\n",
    "    # definitely a grayscale, since some images have a few rgb\n",
    "    # values within the image, which caused problems.\n",
    "    test_X_list = [list(Image.open(scaled_test_dir + image_path).getdata().convert('L')) for image_path, i in test_set_image_paths]\n",
    "    train_X_list = [list(Image.open(scaled_train_dir + image_path).getdata().convert('L')) for image_path, i in train_set_image_paths]\n",
    "\n",
    "    # Downscale the grayscale value for a pixel from [0;255] to [0;1]\n",
    "    test_X_list = [[float(value)/255 for value in image_data] for image_data in test_X_list]\n",
    "    train_X_list = [[float(value)/255 for value in image_data] for image_data in train_X_list]\n",
    "\n",
    "    # Convert both  the test and train lists to actual numpy arrays\n",
    "    train_X = np.array(train_X_list, dtype=float).T\n",
    "    test_X = np.array(test_X_list, dtype=float).T\n",
    "    train_Y = np.array([[float(i) for image_path, i in train_set_image_paths]], dtype=float)\n",
    "    test_Y = np.array([[float(i) for image_path, i in test_set_image_paths]], dtype=float)\n",
    "\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12941176 0.4745098  0.00784314 ... 0.09411765 0.00392157 0.45490196]\n",
      " [0.13333333 0.45490196 0.         ... 0.05490196 0.         0.44705882]\n",
      " [0.14117647 0.42745098 0.         ... 0.1254902  0.00784314 0.45882353]\n",
      " ...\n",
      " [0.14901961 0.10196078 0.         ... 0.12941176 0.         0.01568627]\n",
      " [0.14509804 0.10196078 0.         ... 0.12941176 0.01176471 0.01568627]\n",
      " [0.14117647 0.10196078 0.         ... 0.12941176 0.08235294 0.00392157]]\n",
      "[[0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      "  0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      "  0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      "  1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      "  0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      "  0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      "  1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      "  0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      "  0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      "  0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      "  0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      "  0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      "  1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      "  1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.]]\n",
      "(50176, 5232)\n",
      "(1, 5232)\n",
      "(50176, 624)\n",
      "(1, 624)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "#print(train_X[0:3])\n",
    "#print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    parameters = {}\n",
    "    number_of_layers = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    print(\"Generating a network with the following shapes of the weights and biases:\")\n",
    "    for layer in range(1, number_of_layers):\n",
    "        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1])*0.01\n",
    "        parameters['b'  + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "        \n",
    "        print(\"{}:{}\".format('W' + str(layer), parameters['W' + str(layer)].shape))\n",
    "        print(\"{}:{}\".format('b' + str(layer), parameters['b' + str(layer)].shape))\n",
    "\n",
    "        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))\n",
    "        assert(parameters['b'  + str(layer)].shape == (layer_dims[layer], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    # And implement Forward Propagation to calculate A2 (probabilities)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 8 lines of code)\n",
    "    cache = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    lastA = X\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        Z = np.matmul(parameters['W'+ str(layer)], lastA) + parameters['b' + str(layer)]\n",
    "        A = relu(Z)\n",
    "        cache.update({'Z' + str(layer): Z})\n",
    "        cache.update({'A' + str(layer): A})\n",
    "        lastA = A \n",
    "    Z = np.matmul(parameters['W' + str(number_of_layers+1)], lastA) + parameters['b' + str(number_of_layers+1)]\n",
    "    A = sigmoid(Z)\n",
    "    cache.update({'Z' + str(number_of_layers+1): Z})\n",
    "    cache.update({'A' + str(number_of_layers+1): A})\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    cost = -(1/m)* float(np.dot(Y, np.log(A.T)) + np.dot(1-Y,np.log(1-A.T)))\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # This gives you the cross-entropy part of the cost\n",
    "    cross_entropy_cost = compute_cost(A, Y)\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 line)\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    sum_of_all_weights = 0.0\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        sum_of_all_weights = sum_of_all_weights + np.sum(np.square(parameters['W' + str(layer)]))\n",
    "\n",
    "    L2_regularization_cost = lambd/(2*m) * sum_of_all_weights\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    ### END CODER HERE ###\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*np.matmul(dZ,cache['A' + str(number_of_layers)].T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*np.matmul(dZ,cache['A' + str(layer-1)].T)\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(parameters, cache, X, Y, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*(np.matmul(dZ,cache['A' + str(number_of_layers)].T)+lambd*parameters['W' + str(number_of_layers+1)])\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*(np.matmul(dZ,cache['A' + str(layer-1)].T)+lambd*parameters['W' + str(layer)])\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)    #(1,1)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*(np.matmul(dZ1,X.T)+lambd*parameters['W1'])\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\" and \"grads\"\n",
    "    # Update rule for each parameter\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        parameters['W'+str(layer)] = parameters['W'+str(layer)] - learning_rate*grads['dW'+str(layer)]\n",
    "        parameters['b'+str(layer)] = parameters['b'+str(layer)] - learning_rate*grads['db'+str(layer)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.heaviside(A2-0.5, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_train_accuracy(parameters):\n",
    "    predictions = predict(parameters, train_X)\n",
    "    return float((np.dot(train_Y,predictions.T) + np.dot(1-train_Y,1-predictions.T))/float(train_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(parameters):\n",
    "    predictions = predict(parameters, test_X)\n",
    "    return float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layer_dims, learning_rate, lambd, num_iterations, print_cost=False, print_graph = False, regularization = False):\n",
    "    costs=[]\n",
    "    train_accuracy_values = []\n",
    "    test_accuracy_values = []\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    print(\"\\nStart training:\")\n",
    "    for i in range(num_iterations):\n",
    "        A, cache = forward_propagation(X, parameters)\n",
    "        cost = []\n",
    "        if regularization:\n",
    "            cost = compute_cost_with_regularization(A, Y, parameters, lambd)\n",
    "            print(\"Hello\")\n",
    "        else:\n",
    "            cost = compute_cost(A, Y)\n",
    "        costs.append(cost)\n",
    "        if print_cost == True:\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Cost after iteration {}: {:.2e}\".format(i+1, cost))\n",
    "\n",
    "        grads = {}\n",
    "        if regularization:\n",
    "            grads = backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n",
    "        else:\n",
    "            grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_graph:\n",
    "            train_accuracy_values.append(calculate_train_accuracy(parameters))\n",
    "            test_accuracy_values.append(calculate_test_accuracy(parameters))\n",
    "\n",
    "    \n",
    "    return (parameters, costs, train_accuracy_values, test_accuracy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a network with the following shapes of the weights and biases:\n",
      "W1:(100, 50176)\n",
      "b1:(100, 1)\n",
      "W2:(1, 100)\n",
      "b2:(1, 1)\n",
      "\n",
      "Start training:\n",
      "Cost after iteration 1: 6.71e-01\n",
      "Cost after iteration 2: 6.60e-01\n",
      "Cost after iteration 3: 6.48e-01\n",
      "Cost after iteration 4: 6.38e-01\n",
      "Cost after iteration 5: 6.29e-01\n",
      "Cost after iteration 6: 6.21e-01\n",
      "Cost after iteration 7: 6.14e-01\n",
      "Cost after iteration 8: 6.08e-01\n",
      "Cost after iteration 9: 6.02e-01\n",
      "Cost after iteration 10: 5.98e-01\n",
      "Cost after iteration 11: 5.94e-01\n",
      "Cost after iteration 12: 5.90e-01\n",
      "Cost after iteration 13: 5.87e-01\n",
      "Cost after iteration 14: 5.85e-01\n",
      "Cost after iteration 15: 5.82e-01\n",
      "Cost after iteration 16: 5.81e-01\n",
      "Cost after iteration 17: 5.79e-01\n",
      "Cost after iteration 18: 5.77e-01\n",
      "Cost after iteration 19: 5.76e-01\n",
      "Cost after iteration 20: 5.75e-01\n",
      "Cost after iteration 21: 5.74e-01\n",
      "Cost after iteration 22: 5.73e-01\n",
      "Cost after iteration 23: 5.72e-01\n",
      "Cost after iteration 24: 5.72e-01\n",
      "Cost after iteration 25: 5.71e-01\n",
      "Cost after iteration 26: 5.70e-01\n",
      "Cost after iteration 27: 5.70e-01\n",
      "Cost after iteration 28: 5.70e-01\n",
      "Cost after iteration 29: 5.69e-01\n",
      "Cost after iteration 30: 5.69e-01\n",
      "Cost after iteration 31: 5.68e-01\n",
      "Cost after iteration 32: 5.68e-01\n",
      "Cost after iteration 33: 5.68e-01\n",
      "Cost after iteration 34: 5.67e-01\n",
      "Cost after iteration 35: 5.67e-01\n",
      "Cost after iteration 36: 5.67e-01\n",
      "Cost after iteration 37: 5.67e-01\n",
      "Cost after iteration 38: 5.66e-01\n",
      "Cost after iteration 39: 5.66e-01\n",
      "Cost after iteration 40: 5.66e-01\n",
      "Cost after iteration 41: 5.66e-01\n",
      "Cost after iteration 42: 5.65e-01\n",
      "Cost after iteration 43: 5.65e-01\n",
      "Cost after iteration 44: 5.65e-01\n",
      "Cost after iteration 45: 5.65e-01\n",
      "Cost after iteration 46: 5.65e-01\n",
      "Cost after iteration 47: 5.64e-01\n",
      "Cost after iteration 48: 5.64e-01\n",
      "Cost after iteration 49: 5.64e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [136], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m regularization \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Build a model with a n_h-dimensional hidden layer\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m parameters, costs, train_accuracy_values, test_accuracy_values \u001b[39m=\u001b[39m nn_model(\n\u001b[0;32m     11\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_X, \n\u001b[0;32m     12\u001b[0m     Y\u001b[39m=\u001b[39;49mtrain_Y, \n\u001b[0;32m     13\u001b[0m     layer_dims\u001b[39m=\u001b[39;49mlayer_dims, \n\u001b[0;32m     14\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate, \n\u001b[0;32m     15\u001b[0m     lambd\u001b[39m=\u001b[39;49mlambd,\n\u001b[0;32m     16\u001b[0m     num_iterations \u001b[39m=\u001b[39;49m number_of_iterations, \n\u001b[0;32m     17\u001b[0m     print_cost\u001b[39m=\u001b[39;49mprint_cost, \n\u001b[0;32m     18\u001b[0m     print_graph\u001b[39m=\u001b[39;49mprint_graph, \n\u001b[0;32m     19\u001b[0m     regularization\u001b[39m=\u001b[39;49mregularization\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m plt\u001b[39m.\u001b[39mplot(costs)\n\u001b[0;32m     23\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mcost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [135], line 25\u001b[0m, in \u001b[0;36mnn_model\u001b[1;34m(X, Y, layer_dims, learning_rate, lambd, num_iterations, print_cost, print_graph, regularization)\u001b[0m\n\u001b[0;32m     23\u001b[0m     grads \u001b[39m=\u001b[39m backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n\u001b[0;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     grads \u001b[39m=\u001b[39m backward_propagation(parameters, cache, X, Y)\n\u001b[0;32m     26\u001b[0m parameters \u001b[39m=\u001b[39m update_parameters(parameters, grads, learning_rate)\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m print_graph:\n",
      "Cell \u001b[1;32mIn [129], line 40\u001b[0m, in \u001b[0;36mbackward_propagation\u001b[1;34m(parameters, cache, X, Y)\u001b[0m\n\u001b[0;32m     37\u001b[0m     last_dZ \u001b[39m=\u001b[39m dZ \n\u001b[0;32m     39\u001b[0m dZ1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(parameters[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mT,last_dZ)\u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mheaviside(cache[\u001b[39m'\u001b[39m\u001b[39mA1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m dW1 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(dZ1,X\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m     41\u001b[0m db1 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39msum(dZ1, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m grads\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mdW1\u001b[39m\u001b[39m'\u001b[39m: dW1})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dims = [train_X.shape[0], 100, train_Y.shape[0]]\n",
    "learning_rate = 0.005\n",
    "lambd = 0.2\n",
    "number_of_iterations = 10\n",
    "print_cost = True\n",
    "print_graph = True\n",
    "regularization = False\n",
    "\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs, train_accuracy_values, test_accuracy_values = nn_model(\n",
    "    X=train_X, \n",
    "    Y=train_Y, \n",
    "    layer_dims=layer_dims, \n",
    "    learning_rate=learning_rate, \n",
    "    lambd=lambd,\n",
    "    num_iterations = number_of_iterations, \n",
    "    print_cost=print_cost, \n",
    "    print_graph=print_graph, \n",
    "    regularization=regularization\n",
    ")\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "plt.title(\"alpha: {}; lambda: {}; layers: {}\".format(str(learning_rate), str(lambd), str(layer_dims)))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "if print_graph:\n",
    "    plt.plot(train_accuracy_values, label='train accuracy')\n",
    "    plt.plot(test_accuracy_values, label='test accuracy')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "    plt.title(\"train accuracy and test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
