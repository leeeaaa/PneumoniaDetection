{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "NORMAL_OUTPUT_VALUE = 0\n",
    "PNEUMONIA_OUTPUT_VALUE = 1\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x - A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s - sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x - A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s - relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Collect all the paths of the scaled images and load them into the program\n",
    "    as numpy arrays.\n",
    "\n",
    "    Hint: The original images from https://www.kaggle.com/datasets/tolgadincer/labeled-chest-xray-images\n",
    "    have been downscaled with the sperate python script 'PythonImageScaler.py'\n",
    "\n",
    "    Returns:\n",
    "    train_X - All training images\n",
    "    train_Y - All true outputs of all training images\n",
    "    test_X - All test images\n",
    "    test_Y - All true outputs of all test images\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the root directories of the train and test sets.\n",
    "    scaled_train_dir = \"scaled_chest_xray/train/\"\n",
    "    scaled_test_dir = \"scaled_chest_xray/test/\"\n",
    "\n",
    "    # Define the root directories of the normal and pneumonia \n",
    "    # test and trainsets.\n",
    "    scaled_train_pneu = os.listdir(os.path.join(scaled_train_dir, 'PNEUMONIA'))\n",
    "    scaled_train_normal = os.listdir(os.path.join(scaled_train_dir, 'NORMAL'))\n",
    "    scaled_test_pneu = os.listdir(os.path.join(scaled_test_dir, 'PNEUMONIA'))\n",
    "    scaled_test_normal = os.listdir(os.path.join(scaled_test_dir, 'NORMAL'))\n",
    "\n",
    "    # Collect all paths from the training images and test images\n",
    "    # and combine these with the true output value (either \n",
    "    # NORMAL_OUTPUT_VALUE or PNEUMONIA_OUTPUT_VALUE)\n",
    "    train_set_image_paths = [('PNEUMONIA/' + name, PNEUMONIA_OUTPUT_VALUE) for name in scaled_train_pneu] + [('NORMAL/' + name, NORMAL_OUTPUT_VALUE) for name in scaled_train_normal]\n",
    "    test_set_image_paths = [('PNEUMONIA/' + name, PNEUMONIA_OUTPUT_VALUE) for name in scaled_test_pneu] + [('NORMAL/' + name, NORMAL_OUTPUT_VALUE) for name in scaled_test_normal]\n",
    "\n",
    "    # Shuffle the lists\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(train_set_image_paths) \n",
    "    random.shuffle(test_set_image_paths) \n",
    "\n",
    "    # Open all image paths and read all the grayscale data from every\n",
    "    # grayscale image. \n",
    "    # \".convert('L')\" makes sure that the image is\n",
    "    # definitely a grayscale, since some images have a few rgb\n",
    "    # values within the image, which caused problems.\n",
    "    test_X_list = [list(Image.open(scaled_test_dir + image_path).getdata().convert('L')) for image_path, i in test_set_image_paths]\n",
    "    train_X_list = [list(Image.open(scaled_train_dir + image_path).getdata().convert('L')) for image_path, i in train_set_image_paths]\n",
    "\n",
    "    # Downscale the grayscale value for a pixel from [0;255] to [0;1]\n",
    "    test_X_list = [[float(value)/255 for value in image_data] for image_data in test_X_list]\n",
    "    train_X_list = [[float(value)/255 for value in image_data] for image_data in train_X_list]\n",
    "\n",
    "    # Convert both  the test and train lists to actual numpy arrays\n",
    "    train_X = np.array(train_X_list, dtype=float).T\n",
    "    test_X = np.array(test_X_list, dtype=float).T\n",
    "    train_Y = np.array([[float(i) for image_path, i in train_set_image_paths]], dtype=float)\n",
    "    test_Y = np.array([[float(i) for image_path, i in test_set_image_paths]], dtype=float)\n",
    "\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "#print(train_X[0:3])\n",
    "#print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize all parameters of the neural network based on each layer dimension given\n",
    "    in layer_dims. \n",
    "\n",
    "    Arguments:\n",
    "    layer_dims - python array (list) containing the dimensions of each layer in the network\n",
    "    \n",
    "    Returns:\n",
    "    parameters - python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "        W1 - weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        b1 - bias vector of shape (layer_dims[l], 1)\n",
    "        Wl - weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "        bl - bias vector of shape (1, layer_dims[l])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    parameters = {}\n",
    "    # Number of layers in the network\n",
    "    number_of_layers = len(layer_dims)\n",
    "    \n",
    "    # Generate the all weight and biases matrices for the\n",
    "    # Neural Network based of the values and number of values\n",
    "    # in layer_dims\n",
    "    print(\"Generating a network with the following shapes of the weights and biases:\")\n",
    "    for layer in range(1, number_of_layers):\n",
    "        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1])*0.01\n",
    "        parameters['b'  + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "        \n",
    "        # Print all shapes from all parameters\n",
    "        print(\"{}:{}\".format('W' + str(layer), parameters['W' + str(layer)].shape))\n",
    "        print(\"{}:{}\".format('b' + str(layer), parameters['b' + str(layer)].shape))\n",
    "\n",
    "        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))\n",
    "        assert(parameters['b'  + str(layer)].shape == (layer_dims[layer], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Propagate input features through the neural network.\n",
    "    \n",
    "    Argument:\n",
    "    X - Input features\n",
    "    parameters - Python dictionary containing all parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 - The sigmoid output of the last activation layer\n",
    "    cache - a dictionary containing all values of \"A1\", \"Z1, ... \"AL\", \"ZL\"\n",
    "    \"\"\"\n",
    "    \n",
    "    cache = {}\n",
    "    # Number of layers in the network\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    last_A = X\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        # Calculate Z and A of current layer\n",
    "        Z = np.matmul(parameters['W'+ str(layer)], last_A) + parameters['b' + str(layer)]\n",
    "        A = relu(Z)\n",
    "        # Update cache with Z and A from current layer\n",
    "        cache.update({'Z' + str(layer): Z})\n",
    "        cache.update({'A' + str(layer): A})\n",
    "        # Temporaily save A to use in the next iteration\n",
    "        last_A = A \n",
    "\n",
    "    Z = np.matmul(parameters['W' + str(number_of_layers+1)], last_A) + parameters['b' + str(number_of_layers+1)]\n",
    "    A = sigmoid(Z)\n",
    "    cache.update({'Z' + str(number_of_layers+1): Z})\n",
    "    cache.update({'A' + str(number_of_layers+1): A})\n",
    "    \n",
    "    assert(A.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    cost = -(1/m)* float(np.dot(Y, np.log(A.T)) + np.dot(1-Y,np.log(1-A.T)))\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # This gives you the cross-entropy part of the cost\n",
    "    cross_entropy_cost = compute_cost(A, Y)\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 line)\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    sum_of_all_weights = 0.0\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        sum_of_all_weights = sum_of_all_weights + np.sum(np.square(parameters['W' + str(layer)]))\n",
    "\n",
    "    L2_regularization_cost = lambd/(2*m) * sum_of_all_weights\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    ### END CODER HERE ###\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 (retrieve) + 6 (back prop) lines of code)\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*np.matmul(dZ,cache['A' + str(number_of_layers)].T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*np.matmul(dZ,cache['A' + str(layer-1)].T)\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*np.matmul(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(parameters, cache, X, Y, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "\n",
    "    dZ = cache['A' + str(number_of_layers+1)] - Y\n",
    "    dW = (1/m)*(np.matmul(dZ,cache['A' + str(number_of_layers)].T)+lambd*parameters['W' + str(number_of_layers+1)])\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    grads.update({'dW' + str(number_of_layers+1): dW})\n",
    "    grads.update({'db' + str(number_of_layers+1): db})\n",
    "\n",
    "    last_dZ = dZ\n",
    "    for layer in range(number_of_layers, 1, -1):\n",
    "        dZ = np.matmul(parameters['W' + str(layer+1)].T,last_dZ)* np.heaviside(cache['A' + str(layer)], 1)\n",
    "        dW = (1/m)*(np.matmul(dZ,cache['A' + str(layer-1)].T)+lambd*parameters['W' + str(layer)])\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)    #(1,1)\n",
    "        grads.update({'dW' + str(layer): dW})\n",
    "        grads.update({'db' + str(layer): db})\n",
    "        last_dZ = dZ \n",
    "    \n",
    "    dZ1 = np.matmul(parameters['W2'].T,last_dZ)* np.heaviside(cache['A1'], 1)\n",
    "    dW1 = (1/m)*(np.matmul(dZ1,X.T)+lambd*parameters['W1'])\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims = True)\n",
    "    grads.update({'dW1': dW1})\n",
    "    grads.update({'db1': db1})\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\" and \"grads\"\n",
    "    # Update rule for each parameter\n",
    "    number_of_layers = int(len(parameters)/2)-1\n",
    "    for layer in range(1, number_of_layers+1):\n",
    "        parameters['W'+str(layer)] = parameters['W'+str(layer)] - learning_rate*grads['dW'+str(layer)]\n",
    "        parameters['b'+str(layer)] = parameters['b'+str(layer)] - learning_rate*grads['db'+str(layer)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.heaviside(A2-0.5, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_train_accuracy(parameters):\n",
    "    predictions = predict(parameters, train_X)\n",
    "    return float((np.dot(train_Y,predictions.T) + np.dot(1-train_Y,1-predictions.T))/float(train_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(parameters):\n",
    "    predictions = predict(parameters, test_X)\n",
    "    return float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layer_dims, learning_rate, lambd, num_iterations, print_cost=False, print_graph = False, regularization = False):\n",
    "    costs=[]\n",
    "    train_accuracy_values = []\n",
    "    test_accuracy_values = []\n",
    "\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    print(\"\\nStart training:\")\n",
    "    for i in range(num_iterations):\n",
    "        A, cache = forward_propagation(X, parameters)\n",
    "        cost = 0\n",
    "        if regularization:\n",
    "            cost = compute_cost_with_regularization(A, Y, parameters, lambd)\n",
    "        else:\n",
    "            cost = compute_cost(A, Y)\n",
    "        costs.append(cost)\n",
    "        if print_cost == True:\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Cost after iteration {}: {:.2e}\".format(i+1, cost))\n",
    "\n",
    "        grads = {}\n",
    "        if regularization:\n",
    "            grads = backward_propagation_with_regularization(parameters, cache, X, Y, lambd)\n",
    "        else:\n",
    "            grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_graph:\n",
    "            train_accuracy_values.append(calculate_train_accuracy(parameters))\n",
    "            test_accuracy_values.append(calculate_test_accuracy(parameters))\n",
    "\n",
    "    \n",
    "    return (parameters, costs, train_accuracy_values, test_accuracy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [train_X.shape[0], 100, train_Y.shape[0]]\n",
    "learning_rate = 0.2\n",
    "lambd = 0.2\n",
    "number_of_iterations = 10\n",
    "print_cost = True\n",
    "print_graph = True\n",
    "regularization = False\n",
    "\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs, train_accuracy_values, test_accuracy_values = nn_model(\n",
    "    X=train_X, \n",
    "    Y=train_Y, \n",
    "    layer_dims=layer_dims, \n",
    "    learning_rate=learning_rate, \n",
    "    lambd=lambd,\n",
    "    num_iterations = number_of_iterations, \n",
    "    print_cost=print_cost, \n",
    "    print_graph=print_graph, \n",
    "    regularization=regularization\n",
    ")\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "if regularization:\n",
    "    plt.title(\"alpha: {}; lambda: {}; layers: {}\".format(str(learning_rate), str(lambd), str(layer_dims)))\n",
    "else:\n",
    "    plt.title(\"alpha: {}; layers: {}\".format(str(learning_rate), str(layer_dims)))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "if print_graph:\n",
    "    plt.plot(train_accuracy_values, label='train accuracy')\n",
    "    plt.plot(test_accuracy_values, label='test accuracy')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations: {}'.format(str(number_of_iterations)))\n",
    "    plt.title(\"train accuracy and test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
